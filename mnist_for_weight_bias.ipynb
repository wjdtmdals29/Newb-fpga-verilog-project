{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\2241577816.py:106: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.299814\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.298216\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.318568\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.297652\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.292070\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.279231\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.259009\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.256701\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.266300\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.255168\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.251190\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.262122\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.238704\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.213689\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.241606\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.194288\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.179919\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.104992\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.023214\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 2.004714\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.869187\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.866861\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.592994\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 1.380407\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.053701\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.046393\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.876311\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.770386\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 1.055969\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.796571\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.767476\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.732556\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.626085\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.851731\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.589704\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.621984\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.557334\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.659411\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.429012\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.642014\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.445480\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.442793\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.672738\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.346859\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.546539\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.336138\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.463771\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.584712\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.454444\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.384074\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.439107\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.506627\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.368239\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.327606\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.553716\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.261169\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.362431\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.177567\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.331453\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.313373\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.186171\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.323259\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.349629\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.256305\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.374194\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.221100\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.363095\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.431781\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.428286\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.277041\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.259943\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.347746\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.213425\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.175250\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.127582\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.341840\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.338076\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.175389\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.338450\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.292531\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.209147\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.415320\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.352712\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.387043\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.276871\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.229013\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.385332\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.158784\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.070538\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.311633\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.358583\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.203253\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.327748\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.230561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\2241577816.py:169: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\n",
      "c:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3013, Accuracy: 9047/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.240111\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.161777\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.340523\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.389488\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.099673\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.257463\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.229250\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.308691\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.337729\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.585684\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.322422\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.243561\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.168047\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.207023\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.204207\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.267316\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.361105\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.213788\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.322149\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.386144\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.188990\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.347160\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.281498\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.218941\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.297408\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.200454\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.404295\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.149287\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.294735\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.185155\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.426297\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.172410\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.284018\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.402838\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.230670\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.263754\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.385217\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.302074\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.216568\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.280585\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.279104\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.286383\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.292680\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.260963\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.221816\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.074272\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.156787\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.260529\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.177115\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.163103\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.095381\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.363203\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.324723\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.350918\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.361662\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.261186\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.212280\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.238727\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.159223\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.170796\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.230945\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.209708\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.175901\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.266198\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.296531\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.265355\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.198478\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.199054\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.351767\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.261956\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.208512\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.296555\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.216892\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.116077\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.094261\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.325466\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.218298\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.162621\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.210319\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.214806\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.218360\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.128977\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.110724\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.117733\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.110927\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.230808\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.131644\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.287680\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.221511\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.260436\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.176044\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.181355\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.115186\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.154193\n",
      "\n",
      "Test set: Average loss: 0.2021, Accuracy: 9348/10000 (93%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.264369\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.366187\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.162064\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.262388\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.243741\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.465237\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.119468\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.163366\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.163793\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.270733\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.119988\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.202140\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.137971\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.262887\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.326418\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.297615\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.115551\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.173882\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.457194\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.212352\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.162173\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.253589\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.343722\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.194002\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.224163\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.167196\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.335138\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.285268\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.287250\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.383485\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.182191\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.122485\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.335768\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.225561\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.156001\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.223413\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.207520\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.163290\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.149864\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.193189\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.083624\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.239086\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.067070\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.322711\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.435616\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.080229\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.172181\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.153412\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.381862\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.204367\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.291890\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.702994\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.133283\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.100348\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.138921\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.270880\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.302924\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.130125\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.038408\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.146504\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.155897\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.236119\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.092168\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.177308\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.130443\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.199361\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.160478\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.141658\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.232429\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.142743\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.110405\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.107119\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.141436\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.259260\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.212619\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.107418\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.174724\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.141866\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.129475\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.228404\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.132504\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.121003\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.253294\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.211837\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.054139\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.168285\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.063571\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.167481\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.108343\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.225147\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.111011\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.074575\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.259007\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.235552\n",
      "\n",
      "Test set: Average loss: 0.1470, Accuracy: 9556/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.179598\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.135975\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.086367\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.085867\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.192191\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.198918\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.143760\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.391903\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.097113\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.111399\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.192744\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.122939\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.242250\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.028358\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.098102\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.042946\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.126634\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.221393\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.063355\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.094290\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.041771\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.057326\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.115934\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.091548\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.332081\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.350918\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.126777\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.124387\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.207834\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.177880\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.164328\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.126218\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.051158\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.100932\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.126235\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.151484\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.085639\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.137689\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.116074\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.191501\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.032260\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.121562\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.127986\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.089837\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.115130\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.108954\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.064421\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.214173\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.512338\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.196129\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.186288\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.171934\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.183633\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.184541\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.189832\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.203653\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.146583\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.139594\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.107172\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.366816\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.054401\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.424989\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.047636\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.222498\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.103381\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.112067\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.258453\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.136444\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.133600\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.085545\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.131433\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.122027\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.124708\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.075680\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.051662\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.192100\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.245654\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.177628\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.166489\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.122941\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.113577\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.291059\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.154855\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.217981\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.107447\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.077212\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.089691\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.109755\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.116028\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.347059\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.270477\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.082573\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.208498\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.188970\n",
      "\n",
      "Test set: Average loss: 0.1415, Accuracy: 9566/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.100908\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.257273\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.049445\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.121238\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.136341\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.093553\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.071878\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.148416\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.053851\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.197904\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.077038\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.154547\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.154100\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.118520\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.078603\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.089908\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.127724\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.139750\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.090781\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.122851\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.031700\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.121741\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.166811\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.193624\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.140092\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.208341\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.109680\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.153054\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.155401\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.227071\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.257742\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.056182\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.158103\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.066356\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.180019\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.133592\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.199164\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.086899\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.130263\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.427658\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.212326\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.102046\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.102968\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.181898\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.100693\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.165582\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.223121\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.157602\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.184113\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.160090\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.212329\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.245767\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.303808\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.095521\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.152181\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.126791\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.071462\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.208804\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.155214\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.128686\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.078194\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.138649\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.089337\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.076437\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.309964\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.089088\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.197164\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.103590\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.080603\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.036533\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.056843\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.179466\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.243044\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.170175\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.074342\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.181170\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.218531\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.114389\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.273795\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.141194\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.191509\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.150592\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.037526\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.119904\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.210248\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.047276\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.034134\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.331237\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.085770\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.037394\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.124305\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.026596\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.123794\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.093217\n",
      "\n",
      "Test set: Average loss: 0.1143, Accuracy: 9661/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.032822\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.088261\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.326627\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.067761\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.123658\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.257285\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.052021\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.104599\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.085707\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.203284\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.257259\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.129125\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.078819\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.151424\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.119550\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.103344\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.186530\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.328490\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.092942\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.197314\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.062084\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.142325\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.122261\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.072824\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.265529\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.141922\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.075094\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.058661\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.266275\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.062286\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.160905\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.206915\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.216181\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.071581\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.117283\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.235165\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.095886\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.045938\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.141435\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.328627\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.122163\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.254883\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.108030\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.090830\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.157482\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.273819\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.125264\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.161466\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.107428\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.201520\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.160097\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.165864\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.032325\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.305323\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.100213\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.050483\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.089700\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.115686\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.165284\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.090084\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.150529\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.265333\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.270653\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.182278\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.214514\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.169916\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.060370\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.106637\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.046706\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.052015\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.176369\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.154848\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.088699\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.065681\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.155343\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.143818\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.057253\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.056085\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.101232\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.105320\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.158265\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.204809\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.060704\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.089808\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.076506\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.045549\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.186934\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.118668\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.190092\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.265080\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.105501\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.055229\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.079308\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.054121\n",
      "\n",
      "Test set: Average loss: 0.1158, Accuracy: 9640/10000 (96%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.107524\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.237004\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.095620\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.141955\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.311283\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.255605\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.128096\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.141097\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.045664\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.053499\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.078438\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.275996\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.179860\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.125702\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.230187\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.168371\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.238913\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.131598\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.073564\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.152630\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.191077\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.041322\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.062116\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.125726\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.165392\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.093375\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.261444\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.084774\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.048524\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.022477\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.091605\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.112287\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.179127\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.202641\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.061314\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.304914\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.238416\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.084254\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.117562\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.406229\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.197407\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.112724\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.130181\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.067569\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.118521\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.099897\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.039513\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.177897\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.087537\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.054853\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.042483\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.128248\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.127872\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.064918\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.138473\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.103305\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.092171\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.168042\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.053297\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.181407\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.073784\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.139230\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.037783\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.040525\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.214609\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.083367\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.156953\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.052736\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.065703\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.135669\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.129737\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.067072\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.026737\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.153881\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.064874\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.057326\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.160531\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.417093\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.153895\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.030953\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.118097\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.172585\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.149460\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.143709\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.056375\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.075421\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.099559\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.182126\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.127639\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.121020\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.099717\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.164619\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.050506\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.088307\n",
      "\n",
      "Test set: Average loss: 0.1074, Accuracy: 9655/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.027283\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.061229\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.099157\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.156946\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.065960\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.100100\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.344037\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.129713\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.177443\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.253648\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.222196\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.028661\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.049965\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.044725\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.055341\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.170079\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.052402\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.078784\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.170302\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.177671\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.159337\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.311731\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.168511\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.074135\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.058233\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.058923\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.052376\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.139563\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.280990\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.085182\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.100838\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.436387\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.049345\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.106710\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.136318\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.251443\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.066517\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.054865\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.237707\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.057444\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.116509\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.067077\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.086087\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.089594\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.065184\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.065743\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.165835\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.052211\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.102408\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.056993\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.071166\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.078235\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.052699\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.299074\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.275618\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.117479\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.083703\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.058126\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.205852\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.170113\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.069552\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.063798\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.103752\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.043814\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.142614\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.137679\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.060611\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.056805\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.084635\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.099859\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.058515\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.191093\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.075392\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.149001\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.088875\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.059948\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.054358\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.048844\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.055561\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.081972\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.032626\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.371367\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.155738\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.184270\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.033202\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.035876\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.182786\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.026664\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.290124\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.130819\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.040918\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.184992\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.089390\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.176243\n",
      "\n",
      "Test set: Average loss: 0.1113, Accuracy: 9642/10000 (96%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.185221\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.160309\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.054709\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.052133\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.089157\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.071233\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.132084\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.168725\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.060387\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.165407\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.166752\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.019216\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.182691\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.047324\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.095242\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.107028\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.116646\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.077525\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.058642\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.076209\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.110739\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.058850\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.074372\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.050485\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.218648\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.159890\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.195641\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.034600\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.156220\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.066553\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.146753\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.128867\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.043539\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.086141\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.051451\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.125486\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.059823\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.048625\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.116315\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.023719\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.228288\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.093061\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.098494\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.091698\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.090924\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.163374\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.081386\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.266887\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.143956\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.141401\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.126360\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.058029\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.057636\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.062735\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.061450\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.151384\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.120187\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.239180\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.098936\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.056770\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.158368\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.025408\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.042309\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.049796\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.154624\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.055209\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.046143\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.270723\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.188224\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.029632\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.102666\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.123971\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.058181\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.239345\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.086915\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.047557\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.100254\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.037975\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.025324\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.052215\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.202771\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.077644\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.294313\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.072598\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.070195\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.018724\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.142305\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.276596\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.079143\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.035008\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.123316\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.149031\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.050573\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.055336\n",
      "\n",
      "Test set: Average loss: 0.0921, Accuracy: 9708/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############# MNIST 데이터 로딩 및 신경망 정의##########################\n",
    "\n",
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch.quantization\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "# Training settings\n",
    "batch_size = 64\n",
    "\n",
    "# MNIST Dataset\n",
    "# MNIST 데이터 셋을 가져와 train, test 구분\n",
    "train_dataset = datasets.MNIST(root=\"datasets/\",\n",
    "                                train=True,\n",
    "                                transform=transforms.ToTensor(),\n",
    "                                download=True)\n",
    "test_dataset = datasets.MNIST(root=\"datasets/\",\n",
    "                                train=False,\n",
    "                                transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "# 각 데이터 셋에서 용도에 맞게 데이터를 랜덤하게 섞어서 Batch size 수에 맞게 가져온다\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False)\n",
    "\n",
    "# 신경망 구조 정의\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    # 초기화\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1_out_np = np.zeros((1, 3, 24, 24))\n",
    "        self.mp1_out_np = np.zeros((1, 3, 12, 12))\n",
    "        self.conv2_out_np = np.zeros((3, 3, 8, 8))\n",
    "        #self.conv2_out_np = np.zeros((3, 3, 8, 8))\n",
    "        self.conv3_out_np = np.zeros((1, 3, 8, 8))\n",
    "        #self.conv3_out_np = np.zeros((1, 3, 8, 8))\n",
    "        self.mp2_out_np = np.zeros((1, 3, 4, 4))\n",
    "        #self.mp2_out_np = np.zeros((1, 3, 4, 4))\n",
    "        self.fc_in_np = np.zeros((1, 49))\n",
    "        #self.fc_in_np = np.zeros((1, 48))\n",
    "        self.fc_out_np = np.zeros((1, 10))\n",
    "        \n",
    "        # 커널 크기가 5, 입력 채널이 1, 출력 채널이 3인 CNN 층 구성\n",
    "        # 출력 채널은 커널의 개수를 의미\n",
    "        self.conv1 = nn.Conv2d(1, 3, kernel_size=5, bias=False) # Num of weight = \n",
    "        \n",
    "        # 커널 크기가 5, 입력 채널이 3, 출력 채널이 3인 CNN 층 구성\n",
    "        self.conv2 = nn.Conv2d(3, 3, kernel_size=5, bias=False)\n",
    "        \n",
    "        #self.conv2 = nn.Conv2d(3, 3*6, kernel_size=5, groups=3, bias=False)\n",
    "        \n",
    "        #self.conv3 = nn.Conv2d(3*6, 6, kernel_size=1)\n",
    "        \n",
    "        # Max Pooling Layer, 파라미터는 kernel size 의미, 4개의 값 중 최댓값을 출력한다.\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        self.fc_1 = nn.Linear(48, 10) # Num of Weight = 480\n",
    "        #self.fc_1 = nn.Linear(96, 10) # Num of Weight = 480\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 텐서를 일자로 펴기 위해 x의 한 원소의 크기 계산\n",
    "        in_size = x.size(0)\n",
    "        \n",
    "        # CNN 층 출력이 최대 풀링 층을 지나 활성함수 ReLU를 지난다.\n",
    "        x = self.conv1(x)\n",
    "        self.conv1_out_np = x.detach().numpy()\n",
    "        \n",
    "        x = F.relu(self.mp(x))\n",
    "        self.mp1_out_np = x.detach().numpy()\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        self.conv2_out_np = x.detach().numpy()\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        self.conv3_out_np = x.detach().numpy()\n",
    "        \n",
    "        x = F.relu(self.mp(x))\n",
    "        self.mp2_out_np = x.detach().numpy()\n",
    "        \n",
    "        # 텐서를 Fully Connected Layer에 넣기 위해 일자로 편다\n",
    "        x = x.view(in_size, -1) # flatten the tensor\n",
    "        self.fc_in_np = x.detach().numpy()\n",
    "        \n",
    "        # 일자로 편 텐서로 Fully Connected Layer 계산\n",
    "        x = self.fc_1(x)\n",
    "        self.fc_out_np = x.detach().numpy()\n",
    "        \n",
    "        # 출력층의 활성함수는 Softmax 사용\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "# Instantiation    \n",
    "model = Net()\n",
    "\n",
    "# 최적화 방법으로 Momentum(= 0.5) 방식을 추가한 Stochastical Gradient Descent를 사용한다.\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "################### 신경망 학습 및 실행 ##################################\n",
    "\n",
    "def train(epoch):\n",
    "    # 해당 모델을 training 상태로 설정하는 함수\n",
    "    model.train()\n",
    "    \n",
    "    # train_loader로 batch_idx, data, target 각각 반복마다 \n",
    "    # train_loader의 인덱스를 하나씩 늘려가며 data를 넣는다.\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        #print(batch_idx)\n",
    "        \n",
    "        # Pytorch에서 다룰 수 있게 Variable로 변경\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        #print(np.shape(data))\n",
    "        \n",
    "        # 다음 최적화를 위해 전에 저장되어있던 값을 초기화\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # model을 feed-forward 되어 나온 출력값\n",
    "        output = model(data)\n",
    "        \n",
    "        # Negative Log Likelihood 로 Loss를 계산 \n",
    "        loss = F.nll_loss(output, target)\n",
    "        \n",
    "        # Gradient 값을 계산한다.\n",
    "        loss.backward()\n",
    "        \n",
    "        # back propagation을 통한 가중치값 최적화를 한번 진행\n",
    "        optimizer.step()\n",
    "        \n",
    "        # batch_idx가 10의 배수일 때 => 10번마다\n",
    "        if batch_idx % 10 == 0:\n",
    "            #각 값을 모니터 출력\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                # Percentage로 나타내기 위해 100을 곱함\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "                # data[0] is deprecated\n",
    "\n",
    "# 테스트에 사용할 함수 정의\n",
    "def test():\n",
    "    # 해당 모델을 evaluation 상태로 설정하는 함수\n",
    "    model.eval()\n",
    "    # loss 값 stacking 위해 초기화\n",
    "    test_loss = 0\n",
    "    # 맞춘 수를 stacking 위해 초기화\n",
    "    correct = 0\n",
    "    \n",
    "    \n",
    "    for data, target in test_loader:\n",
    "        \n",
    "        # 각 배열을 PyTorch로 다룰 수 있는 Variable로 만들어준다\n",
    "        # volatile was removed and now has no effect\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        \n",
    "        # model을 feed-forward 되어 나온 출력값\n",
    "        output = model(data)\n",
    "        \n",
    "        # sum up batch loss\n",
    "        #loss 값 stacking\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "        \n",
    "        # get the index of the max log-probability\n",
    "        # max() : 1과 출력 data 중 최대값 출력\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        \n",
    "        # view_as(pred) == view(pred.size())\n",
    "        # => 같은 데이터를 갖는 텐서 배열을 1*n 의 새로운 배열의 텐서로 반환한다.\n",
    "        # cpu() : CUDA 처리가 불가능하여 CPU로 처리하게 하는 함수\n",
    "        # sum() : 텐서의 모든 원소 합 반환\n",
    "        # eq() : 입력받은 두 배열을 비교하여 대응되는 원소값이 갇으면 1,\n",
    "        # 다르면 0을 갖는 같은 크기의 배열을 반환한다\n",
    "        # pred와 target 비교하여 둘이 같으면 1을 갖는 배열의 합, 즉 pred 와 target이 맞는 수를 센다 \n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "    # 데이터 셋의 크기로 Loss를 나눠준다    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    # 각 결과값을 모니터 출력\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0196, 0.0000, 0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0235,\n",
      "           0.0000, 0.0000, 0.0196, 0.0275, 0.0039, 0.0000, 0.0000, 0.0078,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0627, 0.0314, 0.0000,\n",
      "           0.0431, 0.0235, 0.0000, 0.0000, 0.0000, 0.0510, 0.0314, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0549, 0.0078, 0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0510,\n",
      "           0.0000, 0.0000, 0.0039, 0.0275, 0.0157, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0039, 0.0078, 0.0000, 0.0118, 0.0588, 0.0235, 0.0000,\n",
      "           0.0235, 0.0039, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0118,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0157, 0.0157, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0118,\n",
      "           0.0000, 0.0157, 0.0510, 0.0078, 0.0000, 0.0235, 0.0196, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0078, 0.0039, 0.0078, 0.0196, 0.0353, 0.0353, 0.0118, 0.0000,\n",
      "           0.1020, 0.0078, 0.0000, 0.0000, 0.0157, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0078, 0.0000, 0.0314, 0.4471, 0.9490, 1.0000, 0.9216,\n",
      "           0.7529, 0.4157, 0.0824, 0.0000, 0.0000, 0.0118, 0.0118, 0.0314,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.2784, 0.7490, 1.0000, 0.9725, 0.9490, 1.0000,\n",
      "           0.9882, 1.0000, 0.8471, 0.3451, 0.0000, 0.0000, 0.0196, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0078, 0.0000, 0.0549, 0.0000, 0.0000, 0.0039,\n",
      "           0.0471, 0.4627, 0.9922, 0.9333, 1.0000, 0.7686, 0.4471, 0.4353,\n",
      "           0.7255, 0.9490, 1.0000, 0.9765, 0.2863, 0.0000, 0.0000, 0.0157,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0196, 0.0000, 0.0000, 0.0314, 0.0196, 0.0000, 0.0235,\n",
      "           0.3176, 0.9765, 0.9882, 0.9686, 0.2941, 0.0471, 0.0353, 0.0000,\n",
      "           0.0392, 0.0549, 0.6275, 0.9608, 0.8314, 0.0902, 0.0471, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0039, 0.0118, 0.0000, 0.0000, 0.0118, 0.0196, 0.0000, 0.2353,\n",
      "           1.0000, 0.9647, 0.8863, 0.2039, 0.0353, 0.0000, 0.0078, 0.0824,\n",
      "           0.0000, 0.0000, 0.0039, 0.8471, 0.9647, 0.2078, 0.0157, 0.0314,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0078, 0.0000, 0.0000, 0.0353, 0.0039, 0.0000, 0.0235, 0.6314,\n",
      "           0.9843, 0.8314, 0.0784, 0.0824, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0157, 0.0588, 0.1176, 0.8863, 1.0000, 0.2980, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0039, 0.0000, 0.0078, 0.0392, 0.0000, 0.0000, 0.1882, 0.9451,\n",
      "           0.9216, 0.1490, 0.0118, 0.0000, 0.0118, 0.0471, 0.0157, 0.0157,\n",
      "           0.0118, 0.0000, 0.7647, 0.9098, 1.0000, 0.1608, 0.0824, 0.0353,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0078, 0.0000, 0.0000, 0.0627, 0.3804, 1.0000,\n",
      "           0.5255, 0.0000, 0.0471, 0.0000, 0.0039, 0.0157, 0.0000, 0.0118,\n",
      "           0.1608, 0.6431, 0.9961, 1.0000, 0.9882, 0.0784, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0078, 0.0000, 0.0000, 0.0392, 0.4588, 0.9961,\n",
      "           0.1373, 0.0157, 0.0000, 0.0000, 0.0000, 0.1176, 0.3216, 0.6000,\n",
      "           0.9412, 1.0000, 0.9098, 0.9765, 0.1765, 0.0000, 0.0941, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0039, 0.0000, 0.0078, 0.0196, 0.0039, 0.0000, 0.4353, 0.9725,\n",
      "           0.6941, 0.4039, 0.5059, 0.6980, 0.6941, 0.9412, 1.0000, 0.9686,\n",
      "           1.0000, 0.9608, 1.0000, 0.4824, 0.0000, 0.0235, 0.0000, 0.0392,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0078, 0.0000, 0.0039, 0.0196, 0.2667, 0.9608,\n",
      "           0.9882, 1.0000, 0.9804, 1.0000, 1.0000, 0.7294, 0.8627, 0.9569,\n",
      "           0.9765, 1.0000, 0.5059, 0.0000, 0.0039, 0.0627, 0.0000, 0.0078,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0353, 0.0000, 0.0431, 0.0039, 0.0000, 0.0000, 0.0588, 0.5412,\n",
      "           0.9647, 1.0000, 0.7647, 0.8471, 0.5333, 0.0588, 0.0824, 1.0000,\n",
      "           1.0000, 0.9137, 0.1255, 0.0510, 0.0000, 0.0000, 0.0824, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0118, 0.0078, 0.0118, 0.0078, 0.0196, 0.2510,\n",
      "           0.5451, 0.3020, 0.0157, 0.0471, 0.0706, 0.0000, 0.4235, 1.0000,\n",
      "           1.0000, 0.4549, 0.0000, 0.0235, 0.0000, 0.0000, 0.0314, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0235, 0.0000, 0.0275, 0.0000, 0.0000, 0.0157, 0.0000, 0.0392,\n",
      "           0.0000, 0.0000, 0.0431, 0.0000, 0.0000, 0.1255, 1.0000, 0.9922,\n",
      "           0.7608, 0.0078, 0.0549, 0.0000, 0.0275, 0.0353, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0392, 0.0000, 0.0235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0706, 0.0157, 0.0196, 0.0784, 0.6549, 1.0000, 0.9255,\n",
      "           0.3137, 0.0000, 0.0039, 0.0000, 0.0588, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0196, 0.0353, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0863, 0.9412, 0.9765, 0.8784,\n",
      "           0.0157, 0.0353, 0.0000, 0.0157, 0.0000, 0.0000, 0.0039, 0.0353,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0118, 0.0157, 0.0039, 0.0196, 0.0157, 0.0000, 0.0000, 0.0000,\n",
      "           0.0275, 0.0000, 0.0784, 0.0000, 0.2941, 1.0000, 0.9725, 0.3020,\n",
      "           0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0471, 0.0000, 0.0392,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0196, 0.0000, 0.0000, 0.0157,\n",
      "           0.0000, 0.0235, 0.0000, 0.0039, 0.4392, 0.9922, 0.9961, 0.0118,\n",
      "           0.0196, 0.0000, 0.0000, 0.0157, 0.0078, 0.0039, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0157, 0.0196, 1.0000, 0.9686, 0.6000, 0.0235,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0980, 0.0000, 0.0000, 0.1020, 0.6314, 1.0000, 0.1216, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0745, 0.0000, 0.0275, 0.0118, 0.0157, 0.0000, 0.0353,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0510, 0.0196, 0.0196, 0.0000, 0.0353, 0.0000, 0.0078, 0.0235,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x01\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x01\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x07\n",
      "0x0F\n",
      "0x10\n",
      "0x0E\n",
      "0x0C\n",
      "0x06\n",
      "0x01\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x04\n",
      "0x0B\n",
      "0x10\n",
      "0x0F\n",
      "0x0F\n",
      "0x10\n",
      "0x0F\n",
      "0x10\n",
      "0x0D\n",
      "0x05\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x07\n",
      "0x0F\n",
      "0x0E\n",
      "0x10\n",
      "0x0C\n",
      "0x07\n",
      "0x06\n",
      "0x0B\n",
      "0x0F\n",
      "0x10\n",
      "0x0F\n",
      "0x04\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x05\n",
      "0x0F\n",
      "0x0F\n",
      "0x0F\n",
      "0x04\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x0A\n",
      "0x0F\n",
      "0x0D\n",
      "0x01\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x03\n",
      "0x10\n",
      "0x0F\n",
      "0x0E\n",
      "0x03\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x01\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x0D\n",
      "0x0F\n",
      "0x03\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x0A\n",
      "0x0F\n",
      "0x0D\n",
      "0x01\n",
      "0x01\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x01\n",
      "0x0E\n",
      "0x10\n",
      "0x04\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x03\n",
      "0x0F\n",
      "0x0E\n",
      "0x02\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x0C\n",
      "0x0E\n",
      "0x10\n",
      "0x02\n",
      "0x01\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x01\n",
      "0x06\n",
      "0x10\n",
      "0x08\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x02\n",
      "0x0A\n",
      "0x0F\n",
      "0x10\n",
      "0x0F\n",
      "0x01\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x07\n",
      "0x0F\n",
      "0x02\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x01\n",
      "0x05\n",
      "0x09\n",
      "0x0F\n",
      "0x10\n",
      "0x0E\n",
      "0x0F\n",
      "0x02\n",
      "0x00\n",
      "0x01\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x06\n",
      "0x0F\n",
      "0x0B\n",
      "0x06\n",
      "0x08\n",
      "0x0B\n",
      "0x0B\n",
      "0x0F\n",
      "0x10\n",
      "0x0F\n",
      "0x10\n",
      "0x0F\n",
      "0x10\n",
      "0x07\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x04\n",
      "0x0F\n",
      "0x0F\n",
      "0x10\n",
      "0x0F\n",
      "0x10\n",
      "0x10\n",
      "0x0B\n",
      "0x0D\n",
      "0x0F\n",
      "0x0F\n",
      "0x10\n",
      "0x08\n",
      "0x00\n",
      "0x00\n",
      "0x01\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x08\n",
      "0x0F\n",
      "0x10\n",
      "0x0C\n",
      "0x0D\n",
      "0x08\n",
      "0x00\n",
      "0x01\n",
      "0x10\n",
      "0x10\n",
      "0x0E\n",
      "0x02\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x01\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x04\n",
      "0x08\n",
      "0x04\n",
      "0x00\n",
      "0x00\n",
      "0x01\n",
      "0x00\n",
      "0x06\n",
      "0x10\n",
      "0x10\n",
      "0x07\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x02\n",
      "0x10\n",
      "0x0F\n",
      "0x0C\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x01\n",
      "0x00\n",
      "0x00\n",
      "0x01\n",
      "0x0A\n",
      "0x10\n",
      "0x0E\n",
      "0x05\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x01\n",
      "0x0F\n",
      "0x0F\n",
      "0x0E\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x01\n",
      "0x00\n",
      "0x04\n",
      "0x10\n",
      "0x0F\n",
      "0x04\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x07\n",
      "0x0F\n",
      "0x0F\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x10\n",
      "0x0F\n",
      "0x09\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x01\n",
      "0x00\n",
      "0x00\n",
      "0x01\n",
      "0x0A\n",
      "0x10\n",
      "0x01\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x01\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "tensor(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_35728\\2533204397.py:106: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "################# 샘플 통과 후 결과값 추출 #################\n",
    "from PIL import Image\n",
    "\n",
    "# 샘플을 이용한 테스트\n",
    "def sample_test():\n",
    "    # 해당 모델을 evaluation 상태로 설정하는 함수\n",
    "    model.eval()\n",
    "    # loss 값 stacking 위해 초기화\n",
    "    test_loss = 0\n",
    "    # 맞춘 수를 stacking 위해 초기화\n",
    "    correct = 0\n",
    "    \n",
    "    ############## 테스트 샘플 (0 한장) 불러오기 ####################\n",
    "    \n",
    "    # target 생성 (label : 0)\n",
    "    target = Variable(torch.,6]))\n",
    "    \n",
    "    # data 생성 (0_0.bmp)\n",
    "\n",
    "    ######################################\n",
    "   \n",
    "    \n",
    "    img = Image.open(\"C:\\\\Users\\\\user\\\\Desktop\\\\mnist\\\\archive\\\\testSample\\\\testSample\\\\img_205.jpg\", \"r\")\n",
    "    np_img = np.array(img)\n",
    "    np_img_re = np.reshape(np_img, (1,1,28,28))\n",
    "    #print(np_img_re)\n",
    "    #print(Variable(torch.tensor((np_img_re), dtype = torch.float32)))\n",
    "    #np.savetxt('image_test4_num2.mem', np_img_re, fmt='%1.2x',delimiter = \" \")\n",
    "    \n",
    "\n",
    "  \n",
    "     \n",
    "    \n",
    "    \n",
    "    # 0 - 255 => 0 - 1 로 정규화, np.array => tensor 변환\n",
    "    data = Variable(torch.tensor(((np_img_re / 255)), dtype = torch.float32))\n",
    "    float_my = [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0196, 0.0000, 0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0235,\n",
    "           0.0000, 0.0000, 0.0196, 0.0275, 0.0039, 0.0000, 0.0000, 0.0078,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0627, 0.0314, 0.0000,\n",
    "           0.0431, 0.0235, 0.0000, 0.0000, 0.0000, 0.0510, 0.0314, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0549, 0.0078, 0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0510,\n",
    "           0.0000, 0.0000, 0.0039, 0.0275, 0.0157, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0039, 0.0078, 0.0000, 0.0118, 0.0588, 0.0235, 0.0000,\n",
    "           0.0235, 0.0039, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0118,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0157, 0.0157, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0118,\n",
    "           0.0000, 0.0157, 0.0510, 0.0078, 0.0000, 0.0235, 0.0196, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0078, 0.0039, 0.0078, 0.0196, 0.0353, 0.0353, 0.0118, 0.0000,\n",
    "           0.1020, 0.0078, 0.0000, 0.0000, 0.0157, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0078, 0.0000, 0.0314, 0.4471, 0.9490, 1.0000, 0.9216,\n",
    "           0.7529, 0.4157, 0.0824, 0.0000, 0.0000, 0.0118, 0.0118, 0.0314,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.2784, 0.7490, 1.0000, 0.9725, 0.9490, 1.0000,\n",
    "           0.9882, 1.0000, 0.8471, 0.3451, 0.0000, 0.0000, 0.0196, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0078, 0.0000, 0.0549, 0.0000, 0.0000, 0.0039,\n",
    "           0.0471, 0.4627, 0.9922, 0.9333, 1.0000, 0.7686, 0.4471, 0.4353,\n",
    "           0.7255, 0.9490, 1.0000, 0.9765, 0.2863, 0.0000, 0.0000, 0.0157,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0196, 0.0000, 0.0000, 0.0314, 0.0196, 0.0000, 0.0235,\n",
    "           0.3176, 0.9765, 0.9882, 0.9686, 0.2941, 0.0471, 0.0353, 0.0000,\n",
    "           0.0392, 0.0549, 0.6275, 0.9608, 0.8314, 0.0902, 0.0471, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0039, 0.0118, 0.0000, 0.0000, 0.0118, 0.0196, 0.0000, 0.2353,\n",
    "           1.0000, 0.9647, 0.8863, 0.2039, 0.0353, 0.0000, 0.0078, 0.0824,\n",
    "           0.0000, 0.0000, 0.0039, 0.8471, 0.9647, 0.2078, 0.0157, 0.0314,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0078, 0.0000, 0.0000, 0.0353, 0.0039, 0.0000, 0.0235, 0.6314,\n",
    "           0.9843, 0.8314, 0.0784, 0.0824, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0157, 0.0588, 0.1176, 0.8863, 1.0000, 0.2980, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0039, 0.0000, 0.0078, 0.0392, 0.0000, 0.0000, 0.1882, 0.9451,\n",
    "           0.9216, 0.1490, 0.0118, 0.0000, 0.0118, 0.0471, 0.0157, 0.0157,\n",
    "           0.0118, 0.0000, 0.7647, 0.9098, 1.0000, 0.1608, 0.0824, 0.0353,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0078, 0.0000, 0.0000, 0.0627, 0.3804, 1.0000,\n",
    "           0.5255, 0.0000, 0.0471, 0.0000, 0.0039, 0.0157, 0.0000, 0.0118,\n",
    "           0.1608, 0.6431, 0.9961, 1.0000, 0.9882, 0.0784, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0078, 0.0000, 0.0000, 0.0392, 0.4588, 0.9961,\n",
    "           0.1373, 0.0157, 0.0000, 0.0000, 0.0000, 0.1176, 0.3216, 0.6000,\n",
    "           0.9412, 1.0000, 0.9098, 0.9765, 0.1765, 0.0000, 0.0941, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0039, 0.0000, 0.0078, 0.0196, 0.0039, 0.0000, 0.4353, 0.9725,\n",
    "           0.6941, 0.4039, 0.5059, 0.6980, 0.6941, 0.9412, 1.0000, 0.9686,\n",
    "           1.0000, 0.9608, 1.0000, 0.4824, 0.0000, 0.0235, 0.0000, 0.0392,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0078, 0.0000, 0.0039, 0.0196, 0.2667, 0.9608,\n",
    "           0.9882, 1.0000, 0.9804, 1.0000, 1.0000, 0.7294, 0.8627, 0.9569,\n",
    "           0.9765, 1.0000, 0.5059, 0.0000, 0.0039, 0.0627, 0.0000, 0.0078,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0353, 0.0000, 0.0431, 0.0039, 0.0000, 0.0000, 0.0588, 0.5412,\n",
    "           0.9647, 1.0000, 0.7647, 0.8471, 0.5333, 0.0588, 0.0824, 1.0000,\n",
    "           1.0000, 0.9137, 0.1255, 0.0510, 0.0000, 0.0000, 0.0824, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0118, 0.0078, 0.0118, 0.0078, 0.0196, 0.2510,\n",
    "           0.5451, 0.3020, 0.0157, 0.0471, 0.0706, 0.0000, 0.4235, 1.0000,\n",
    "           1.0000, 0.4549, 0.0000, 0.0235, 0.0000, 0.0000, 0.0314, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0235, 0.0000, 0.0275, 0.0000, 0.0000, 0.0157, 0.0000, 0.0392,\n",
    "           0.0000, 0.0000, 0.0431, 0.0000, 0.0000, 0.1255, 1.0000, 0.9922,\n",
    "           0.7608, 0.0078, 0.0549, 0.0000, 0.0275, 0.0353, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0392, 0.0000, 0.0235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0706, 0.0157, 0.0196, 0.0784, 0.6549, 1.0000, 0.9255,\n",
    "           0.3137, 0.0000, 0.0039, 0.0000, 0.0588, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0196, 0.0353, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0863, 0.9412, 0.9765, 0.8784,\n",
    "           0.0157, 0.0353, 0.0000, 0.0157, 0.0000, 0.0000, 0.0039, 0.0353,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0118, 0.0157, 0.0039, 0.0196, 0.0157, 0.0000, 0.0000, 0.0000,\n",
    "           0.0275, 0.0000, 0.0784, 0.0000, 0.2941, 1.0000, 0.9725, 0.3020,\n",
    "           0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0471, 0.0000, 0.0392,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0196, 0.0000, 0.0000, 0.0157,\n",
    "           0.0000, 0.0235, 0.0000, 0.0039, 0.4392, 0.9922, 0.9961, 0.0118,\n",
    "           0.0196, 0.0000, 0.0000, 0.0157, 0.0078, 0.0039, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0157, 0.0196, 1.0000, 0.9686, 0.6000, 0.0235,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0980, 0.0000, 0.0000, 0.1020, 0.6314, 1.0000, 0.1216, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0745, 0.0000, 0.0275, 0.0118, 0.0157, 0.0000, 0.0353,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0510, 0.0196, 0.0196, 0.0000, 0.0353, 0.0000, 0.0078, 0.0235,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000]]\n",
    "    \n",
    "    print(data)\n",
    "    from fxpmath import Fxp\n",
    "    #b = Fxp(1.1445, signed=True, n_word=8, n_frac=4)\n",
    "    #print(b.hex())\n",
    "\n",
    "    for i in range(28):\n",
    "        for j in range(28):\n",
    "        \n",
    "            x = Fxp(float_my[i][j], signed=True, n_word=8, n_frac=4)\n",
    "            #print(x)         # float value\n",
    "            #print(x.bin())   # binary representation\n",
    "            print(x.hex())   # hex repr\n",
    "            #print(x.val)     # raw val (decimal of binary stored)\n",
    "            \n",
    "    \n",
    "    # model을 feed-forward 되어 나온 출력값\n",
    "    output = model(data)\n",
    "    #print(np.shape(output))\n",
    "    \n",
    "    # sum up batch loss\n",
    "    # loss 값 stacking\n",
    "    test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "    \n",
    "    \n",
    "    # get the index of the max log-probability\n",
    "    # max() : 1과 출력 data 중 최대값 출력\n",
    "    pred = output.data.max(1, keepdim=True)[1]\n",
    "    \n",
    "    #view_as(pred) == view(pred.size())\n",
    "    # => 같은 데이터를 갖는 텐서 배열을 1*n 의 새로운 배열의 텐서로 반환한다.\n",
    "    # cpu() : CUDA 처리가 불가능하여 CPU로 처리하게 하는 함수\n",
    "    \n",
    "    # sum() : 텐서의 모든 원소 합 반환\n",
    "    # eq() : 입력받은 두 배열을 비교하여 대응되는 원소값이 갇으면 1,\n",
    "    # 다르면 0을 갖는 같은 크기의 배열을 반환한다\n",
    "    # pred와 target 비교하여 둘이 같으면 1을 갖는 배열의 합, 즉 pred 와 target이 맞는 수를 센다 \n",
    "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    print(correct)\n",
    "    \n",
    "    # 데이터 셋의 크기로 Loss를 나눠준다    \n",
    "    #test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    # 각 결과값을 모니터 출력\n",
    "    #print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        #test_loss, correct, len(test_loader.dataset),\n",
    "        #100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "\n",
    "sample_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0773,  0.2015,  0.1873, -0.0046,  0.1555],\n",
      "        [-0.0696, -0.2170, -0.0775,  0.1051,  0.2148],\n",
      "        [ 0.0901, -0.1994, -0.0626,  0.1994,  0.3410],\n",
      "        [-0.0999, -0.0086, -0.0725,  0.1980,  0.2159],\n",
      "        [ 0.0177, -0.1017, -0.1570, -0.0096,  0.2055]])\n",
      "tensor([[-0.0953,  0.0423,  0.3730,  0.2413,  0.0263],\n",
      "        [-0.1221,  0.1831,  0.2278,  0.3930,  0.2553],\n",
      "        [ 0.1548,  0.3197,  0.5521,  0.8877,  0.4599],\n",
      "        [ 0.0680,  0.5098,  0.8524,  0.5987,  0.1161],\n",
      "        [ 0.1649,  0.1793,  0.4654,  0.3942,  0.2900]])\n",
      "tensor([[ 0.2414,  0.0408, -0.2019,  0.0548, -0.1531],\n",
      "        [ 0.1850,  0.3362,  0.5191,  0.5919,  0.4848],\n",
      "        [-0.0308,  0.4638,  0.5315,  0.3755,  0.2551],\n",
      "        [-0.1318, -0.3182, -0.1797, -0.0086,  0.0633],\n",
      "        [-0.2279, -0.5504, -0.6544, -0.3949, -0.0072]])\n",
      "weights\n",
      "0xFF\n",
      "0x03\n",
      "0x02\n",
      "0x00\n",
      "0x02\n",
      "0xFF\n",
      "0xFD\n",
      "0xFF\n",
      "0x01\n",
      "0x03\n",
      "0x01\n",
      "0xFD\n",
      "0xFF\n",
      "0x03\n",
      "0x05\n",
      "0xFF\n",
      "0x00\n",
      "0xFF\n",
      "0x03\n",
      "0x03\n",
      "0x00\n",
      "0xFF\n",
      "0xFE\n",
      "0x00\n",
      "0x03\n",
      "0xFF\n",
      "0x00\n",
      "0x05\n",
      "0x03\n",
      "0x00\n",
      "0xFF\n",
      "0x02\n",
      "0x03\n",
      "0x06\n",
      "0x04\n",
      "0x02\n",
      "0x05\n",
      "0x08\n",
      "0x0E\n",
      "0x07\n",
      "0x01\n",
      "0x08\n",
      "0x0D\n",
      "0x09\n",
      "0x01\n",
      "0x02\n",
      "0x02\n",
      "0x07\n",
      "0x06\n",
      "0x04\n",
      "0x03\n",
      "0x00\n",
      "0xFD\n",
      "0x00\n",
      "0xFE\n",
      "0x02\n",
      "0x05\n",
      "0x08\n",
      "0x09\n",
      "0x07\n",
      "0x00\n",
      "0x07\n",
      "0x08\n",
      "0x06\n",
      "0x04\n",
      "0xFE\n",
      "0xFB\n",
      "0xFE\n",
      "0x00\n",
      "0x01\n",
      "0xFD\n",
      "0xF8\n",
      "0xF6\n",
      "0xFA\n",
      "0x00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\2863423040.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv1_weight_1 =  torch.tensor((model.conv1.weight.data[0][0]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\2863423040.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv1_weight_2 =  torch.tensor((model.conv1.weight.data[1][0]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\2863423040.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv1_weight_3 =  torch.tensor((model.conv1.weight.data[2][0]), dtype = torch.float32)\n"
     ]
    }
   ],
   "source": [
    "############## Conv1 가중치 값 HEX 추출 ############\n",
    "\n",
    "# float => int\n",
    "conv1_weight =  [[-0.0773,  0.2015,  0.1873, -0.0046,  0.1555],\n",
    "        [-0.0696, -0.2170, -0.0775,  0.1051,  0.2148],\n",
    "        [ 0.0901, -0.1994, -0.0626,  0.1994,  0.3410],\n",
    "        [-0.0999, -0.0086, -0.0725,  0.1980,  0.2159],\n",
    "        [ 0.0177, -0.1017, -0.1570, -0.0096,  0.2055],\n",
    "[-0.0953,  0.0423,  0.3730,  0.2413,  0.0263],\n",
    "        [-0.1221,  0.1831,  0.2278,  0.3930,  0.2553],\n",
    "        [ 0.1548,  0.3197,  0.5521,  0.8877,  0.4599],\n",
    "        [ 0.0680,  0.5098,  0.8524,  0.5987,  0.1161],\n",
    "        [ 0.1649,  0.1793,  0.4654,  0.3942,  0.2900],\n",
    "[ 0.2414,  0.0408, -0.2019,  0.0548, -0.1531],\n",
    "        [ 0.1850,  0.3362,  0.5191,  0.5919,  0.4848],\n",
    "        [-0.0308,  0.4638,  0.5315,  0.3755,  0.2551],\n",
    "        [-0.1318, -0.3182, -0.1797, -0.0086,  0.0633],\n",
    "        [-0.2279, -0.5504, -0.6544, -0.3949, -0.0072]]\n",
    "\n",
    "\n",
    "float_conv1_weight_1 =  torch.tensor((model.conv1.weight.data[0][0]), dtype = torch.float32)\n",
    "float_conv1_weight_2 =  torch.tensor((model.conv1.weight.data[1][0]), dtype = torch.float32)\n",
    "float_conv1_weight_3 =  torch.tensor((model.conv1.weight.data[2][0]), dtype = torch.float32)\n",
    "\n",
    "arrary_conv1_weight = np.zeros((15,5))\n",
    "\n",
    "#print(\"Signed\")\n",
    "print(float_conv1_weight_1)\n",
    "print(float_conv1_weight_2)\n",
    "print(float_conv1_weight_3)\n",
    "\n",
    "from fxpmath import Fxp\n",
    "   \n",
    "print(\"weights\")\n",
    "for i in range(15):\n",
    "    for j in range(5):\n",
    "        \n",
    "        x = Fxp(conv1_weight[i][j], signed=True, n_word=8, n_frac=4)\n",
    "        #print(x)\n",
    "        arrary_conv1_weight[i][j] = x.bin(); \n",
    "        # # float value\n",
    "        #print(x.bin())   # binary representation\n",
    "        print(x.hex())   # hex repr\n",
    "        #print(x.val)     # raw val (decimal of binary stored)\n",
    "#print(int_conv1_weight_2)\n",
    "#print(int_conv1_weight_3)\n",
    "#print(int_conv1_weight_1)\n",
    "#print(arrary_conv1_weight)\n",
    "\n",
    "# signed int => unsigned int\n",
    "#for i in range(5):\n",
    "    #for j in range(5):\n",
    "       \n",
    "        #if int_conv1_weight_1[i][j] < 0:\n",
    "        #    int_conv1_weight_1[i][j] += 256\n",
    "        #if int_conv1_weight_2[i][j] < 0:\n",
    "        #    int_conv1_weight_2[i][j] += 256\n",
    "        #if int_conv1_weight_3[i][j] < 0:\n",
    "        #    int_conv1_weight_3[i][j] += 256\n",
    "\n",
    "#for i in range(3):\n",
    " #   if int_conv1_bias[i] < 0:\n",
    "  #          int_conv1_bias[i] += 256*256\n",
    "#\n",
    "#print (\"Unsigned\")\n",
    "#print(int_conv1_weight_1)\n",
    "#print(int_conv1_weight_2)\n",
    "#print(int_conv1_weight_3)\n",
    "#print(int_conv1_bias)\n",
    "#np.savetxt('conv1_weight_1.mem', int_conv1_weight_1, fmt='%1.2x',delimiter = \" \")\n",
    "#np.savetxt('conv1_weight_2.mem', int_conv1_weight_2, fmt='%1.2x',delimiter = \" \")\n",
    "#np.savetxt('conv1_weight_3.mem', int_conv1_weight_3, fmt='%1.2x',delimiter = \" \")\n",
    "#np.savetxt('conv1_bias.mem', int_conv1_bias, fmt='%1.2x',delimiter = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 1, 5, 5])\n",
      "tensor([[ 0.1184,  0.1777, -0.0042, -0.1301, -0.1472],\n",
      "        [-0.1608, -0.1476,  0.1198,  0.0973,  0.0907],\n",
      "        [ 0.1599, -0.0432,  0.1043, -0.1840,  0.0241],\n",
      "        [-0.1670, -0.0661, -0.0222, -0.1046, -0.0475],\n",
      "        [ 0.1665,  0.0124, -0.1940,  0.0043, -0.1785]])\n",
      "tensor([[ 0.1317, -0.0243,  0.1257,  0.2123, -0.0665],\n",
      "        [-0.1901,  0.0699,  0.2054,  0.0943,  0.1152],\n",
      "        [-0.0420,  0.0330,  0.2671, -0.0268,  0.1289],\n",
      "        [ 0.1097,  0.1125,  0.0085,  0.1843, -0.1247],\n",
      "        [ 0.1298,  0.1431,  0.0524,  0.1489,  0.1840]])\n",
      "tensor([[-0.2198,  0.0707,  0.2381,  0.0219, -0.1041],\n",
      "        [-0.1509,  0.1441,  0.1081,  0.1325, -0.1573],\n",
      "        [-0.0587, -0.0417,  0.2373,  0.0116, -0.0945],\n",
      "        [-0.0157, -0.0278, -0.0492, -0.0701,  0.1609],\n",
      "        [-0.0615, -0.0781, -0.0163,  0.2394,  0.1353]])\n",
      "tensor([[-0.0962,  0.0199,  0.2266, -0.1618, -0.0050],\n",
      "        [-0.0727,  0.0810,  0.1946, -0.1478, -0.0307],\n",
      "        [ 0.0315,  0.1458, -0.0004, -0.0993, -0.1629],\n",
      "        [ 0.1195, -0.1430,  0.1795, -0.1107, -0.0704],\n",
      "        [-0.0971, -0.1640,  0.0645,  0.1642,  0.0202]])\n",
      "tensor([[ 0.0505,  0.0796,  0.0385,  0.0586, -0.1782],\n",
      "        [-0.0611, -0.0952, -0.2513, -0.0485,  0.2246],\n",
      "        [-0.0773, -0.1652, -0.0665,  0.0073,  0.2135],\n",
      "        [-0.0557, -0.0851, -0.2560, -0.2050, -0.0694],\n",
      "        [ 0.1011, -0.1036,  0.1115,  0.0099, -0.1640]])\n",
      "tensor([[ 0.1290, -0.1563,  0.1341,  0.1788, -0.0821],\n",
      "        [-0.1473, -0.1850,  0.1907,  0.1456,  0.1235],\n",
      "        [ 0.0419,  0.0074,  0.0123,  0.0296,  0.0577],\n",
      "        [-0.1207,  0.0259, -0.1164, -0.0834, -0.0293],\n",
      "        [ 0.0425, -0.1879, -0.1615, -0.0523,  0.1819]])\n",
      "tensor([[-0.2999, -0.2512, -0.3695, -0.3287, -0.1825],\n",
      "        [ 0.0784,  0.1102,  0.0060, -0.1921, -0.0706],\n",
      "        [-0.0723,  0.3526,  0.2258,  0.2869,  0.2250],\n",
      "        [-0.0418,  0.0080, -0.1967, -0.2840,  0.1730],\n",
      "        [-0.0536, -0.0408,  0.0718, -0.1427,  0.0544]])\n",
      "tensor([[-0.1292, -0.1552,  0.1568, -0.0277, -0.1359],\n",
      "        [-0.0848,  0.2838,  0.1847, -0.3486, -0.1957],\n",
      "        [ 0.1465,  0.1529, -0.2023, -0.2469,  0.2335],\n",
      "        [-0.1802,  0.1978, -0.0155, -0.1185,  0.0976],\n",
      "        [-0.0020, -0.0766,  0.2636, -0.0435,  0.3286]])\n",
      "tensor([[ 0.1341, -0.2531, -0.3342, -0.2920, -0.1205],\n",
      "        [-0.2182, -0.0063, -0.0346, -0.2450, -0.1592],\n",
      "        [ 0.1975,  0.3989,  0.0252, -0.2345,  0.0569],\n",
      "        [ 0.1997, -0.1044, -0.0140, -0.1917,  0.0346],\n",
      "        [-0.0273, -0.1904, -0.1436,  0.1506,  0.0546]])\n",
      "tensor([[ 0.1614, -0.0780,  0.1382,  0.1170, -0.0392],\n",
      "        [ 0.0955,  0.1066,  0.0379, -0.2111, -0.1429],\n",
      "        [ 0.1930,  0.0045, -0.3711, -0.2764, -0.2159],\n",
      "        [ 0.2218, -0.0204, -0.2184,  0.0313, -0.0597],\n",
      "        [ 0.1819,  0.0733,  0.1990, -0.1016,  0.2146]])\n",
      "tensor([[ 0.0509,  0.2499,  0.2172,  0.0388,  0.1408],\n",
      "        [ 0.0590,  0.0518,  0.0766,  0.3883,  0.1145],\n",
      "        [-0.3542, -0.4204,  0.0768,  0.2111, -0.0452],\n",
      "        [-0.1748, -0.5332, -0.2188,  0.3843, -0.0563],\n",
      "        [-0.0901, -0.0735, -0.0298,  0.2214, -0.1340]])\n",
      "tensor([[ 0.2469,  0.2628,  0.1577,  0.2869,  0.0428],\n",
      "        [-0.0219,  0.1019,  0.1583, -0.1550, -0.0962],\n",
      "        [-0.2320, -0.4619, -0.6237, -0.5795, -0.1699],\n",
      "        [-0.3245, -0.2049, -0.0837,  0.1662,  0.0400],\n",
      "        [-0.0076,  0.2074,  0.2341,  0.3290, -0.0857]])\n",
      "tensor([[-0.0383,  0.2112,  0.3010, -0.0745,  0.0316],\n",
      "        [ 0.2390,  0.2874, -0.0581,  0.0664,  0.1628],\n",
      "        [-0.1148,  0.0006, -0.0919,  0.2606, -0.0914],\n",
      "        [-0.0016,  0.2269,  0.2143, -0.0500, -0.1143],\n",
      "        [ 0.1794,  0.3036,  0.1181, -0.1664,  0.1184]])\n",
      "tensor([[ 0.1319,  0.0015,  0.1172,  0.0875,  0.0030],\n",
      "        [-0.0456,  0.0598, -0.1365, -0.0250, -0.1217],\n",
      "        [-0.1414, -0.1543,  0.0386, -0.0426, -0.2418],\n",
      "        [ 0.1072, -0.2245,  0.1625,  0.0802,  0.1022],\n",
      "        [-0.1416, -0.1283, -0.1646, -0.0346,  0.0324]])\n",
      "tensor([[-0.0181,  0.1153, -0.0137,  0.1152, -0.0995],\n",
      "        [-0.1205,  0.2284,  0.3136,  0.4492,  0.2332],\n",
      "        [-0.1927,  0.2196, -0.0120, -0.0824,  0.0420],\n",
      "        [-0.1457,  0.0015, -0.1079, -0.1756, -0.1657],\n",
      "        [ 0.0151,  0.1027,  0.2652,  0.0101, -0.1275]])\n",
      "tensor([[-0.2177, -0.2263, -0.1631, -0.2365, -0.1516],\n",
      "        [-0.1785, -0.2252, -0.5882, -0.6372, -0.1528],\n",
      "        [ 0.1287, -0.2361, -0.2780, -0.1335, -0.1741],\n",
      "        [ 0.1401,  0.0814,  0.2366, -0.1604,  0.1257],\n",
      "        [ 0.2126,  0.0126,  0.0569,  0.0815,  0.2374]])\n",
      "tensor([[-0.0626, -0.1978, -0.3155,  0.0076, -0.2358],\n",
      "        [-0.1150, -0.1860, -0.1308, -0.2000, -0.2655],\n",
      "        [-0.1086,  0.0929,  0.0455, -0.0445, -0.0717],\n",
      "        [-0.1809,  0.0684,  0.0342,  0.2559,  0.2426],\n",
      "        [-0.2472, -0.3982, -0.1720,  0.1056,  0.1337]])\n",
      "tensor([[-0.2339, -0.1244,  0.0774,  0.2545, -0.1562],\n",
      "        [-0.0712, -0.2176, -0.1240,  0.1255, -0.1296],\n",
      "        [ 0.2199, -0.0355,  0.2808, -0.0167,  0.1841],\n",
      "        [-0.0695,  0.1678,  0.1806, -0.0723, -0.0707],\n",
      "        [ 0.0850,  0.2393, -0.0052,  0.1778,  0.1152]])\n",
      "0x01\n",
      "0x02\n",
      "0x00\n",
      "0xFE\n",
      "0xFE\n",
      "0xFE\n",
      "0xFE\n",
      "0x01\n",
      "0x01\n",
      "0x01\n",
      "0x02\n",
      "0x00\n",
      "0x01\n",
      "0xFE\n",
      "0x00\n",
      "0xFE\n",
      "0xFF\n",
      "0x00\n",
      "0xFF\n",
      "0x00\n",
      "0x02\n",
      "0x00\n",
      "0xFD\n",
      "0x00\n",
      "0xFE\n",
      "0x02\n",
      "0x00\n",
      "0x02\n",
      "0x03\n",
      "0xFF\n",
      "0xFD\n",
      "0x01\n",
      "0x03\n",
      "0x01\n",
      "0x01\n",
      "0x00\n",
      "0x00\n",
      "0x04\n",
      "0x00\n",
      "0x02\n",
      "0x01\n",
      "0x01\n",
      "0x00\n",
      "0x02\n",
      "0xFF\n",
      "0x02\n",
      "0x02\n",
      "0x00\n",
      "0x02\n",
      "0x02\n",
      "0xFD\n",
      "0x01\n",
      "0x03\n",
      "0x00\n",
      "0xFF\n",
      "0xFE\n",
      "0x02\n",
      "0x01\n",
      "0x02\n",
      "0xFE\n",
      "0x00\n",
      "0x00\n",
      "0x03\n",
      "0x00\n",
      "0xFF\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0xFF\n",
      "0x02\n",
      "0x00\n",
      "0xFF\n",
      "0x00\n",
      "0x03\n",
      "0x02\n",
      "0xFF\n",
      "0x00\n",
      "0x03\n",
      "0xFE\n",
      "0x00\n",
      "0xFF\n",
      "0x01\n",
      "0x03\n",
      "0xFE\n",
      "0x00\n",
      "0x00\n",
      "0x02\n",
      "0x00\n",
      "0xFF\n",
      "0xFE\n",
      "0x01\n",
      "0xFE\n",
      "0x02\n",
      "0xFF\n",
      "0xFF\n",
      "0xFF\n",
      "0xFE\n",
      "0x01\n",
      "0x02\n",
      "0x00\n",
      "0x00\n",
      "0x01\n",
      "0x00\n",
      "0x00\n",
      "0xFE\n",
      "0x00\n",
      "0xFF\n",
      "0xFC\n",
      "0x00\n",
      "0x03\n",
      "0xFF\n",
      "0xFE\n",
      "0xFF\n",
      "0x00\n",
      "0x03\n",
      "0x00\n",
      "0xFF\n",
      "0xFC\n",
      "0xFD\n",
      "0xFF\n",
      "0x01\n",
      "0xFF\n",
      "0x01\n",
      "0x00\n",
      "0xFE\n",
      "0x02\n",
      "0xFE\n",
      "0x02\n",
      "0x02\n",
      "0xFF\n",
      "0xFE\n",
      "0xFE\n",
      "0x03\n",
      "0x02\n",
      "0x01\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0xFF\n",
      "0x00\n",
      "0xFF\n",
      "0xFF\n",
      "0x00\n",
      "0x00\n",
      "0xFD\n",
      "0xFE\n",
      "0x00\n",
      "0x02\n",
      "0xFC\n",
      "0xFC\n",
      "0xFB\n",
      "0xFB\n",
      "0xFE\n",
      "0x01\n",
      "0x01\n",
      "0x00\n",
      "0xFD\n",
      "0xFF\n",
      "0xFF\n",
      "0x05\n",
      "0x03\n",
      "0x04\n",
      "0x03\n",
      "0x00\n",
      "0x00\n",
      "0xFD\n",
      "0xFC\n",
      "0x02\n",
      "0x00\n",
      "0x00\n",
      "0x01\n",
      "0xFE\n",
      "0x00\n",
      "0xFE\n",
      "0xFE\n",
      "0x02\n",
      "0x00\n",
      "0xFE\n",
      "0xFF\n",
      "0x04\n",
      "0x02\n",
      "0xFB\n",
      "0xFD\n",
      "0x02\n",
      "0x02\n",
      "0xFD\n",
      "0xFD\n",
      "0x03\n",
      "0xFE\n",
      "0x03\n",
      "0x00\n",
      "0xFF\n",
      "0x01\n",
      "0x00\n",
      "0xFF\n",
      "0x04\n",
      "0x00\n",
      "0x05\n",
      "0x02\n",
      "0xFC\n",
      "0xFB\n",
      "0xFC\n",
      "0xFF\n",
      "0xFD\n",
      "0x00\n",
      "0x00\n",
      "0xFD\n",
      "0xFE\n",
      "0x03\n",
      "0x06\n",
      "0x00\n",
      "0xFD\n",
      "0x00\n",
      "0x03\n",
      "0xFF\n",
      "0x00\n",
      "0xFD\n",
      "0x00\n",
      "0x00\n",
      "0xFD\n",
      "0xFE\n",
      "0x02\n",
      "0x00\n",
      "0x02\n",
      "0xFF\n",
      "0x02\n",
      "0x01\n",
      "0x00\n",
      "0x01\n",
      "0x01\n",
      "0x00\n",
      "0xFD\n",
      "0xFE\n",
      "0x03\n",
      "0x00\n",
      "0xFB\n",
      "0xFC\n",
      "0xFD\n",
      "0x03\n",
      "0x00\n",
      "0xFD\n",
      "0x00\n",
      "0x00\n",
      "0x02\n",
      "0x01\n",
      "0x03\n",
      "0xFF\n",
      "0x03\n",
      "0x00\n",
      "0x03\n",
      "0x03\n",
      "0x00\n",
      "0x02\n",
      "0x00\n",
      "0x00\n",
      "0x01\n",
      "0x06\n",
      "0x01\n",
      "0xFB\n",
      "0xFA\n",
      "0x01\n",
      "0x03\n",
      "0x00\n",
      "0xFE\n",
      "0xF8\n",
      "0xFD\n",
      "0x06\n",
      "0x00\n",
      "0xFF\n",
      "0xFF\n",
      "0x00\n",
      "0x03\n",
      "0xFE\n",
      "0x03\n",
      "0x04\n",
      "0x02\n",
      "0x04\n",
      "0x00\n",
      "0x00\n",
      "0x01\n",
      "0x02\n",
      "0xFE\n",
      "0xFF\n",
      "0xFD\n",
      "0xF9\n",
      "0xF7\n",
      "0xF7\n",
      "0xFE\n",
      "0xFB\n",
      "0xFD\n",
      "0xFF\n",
      "0x02\n",
      "0x00\n",
      "0x00\n",
      "0x03\n",
      "0x03\n",
      "0x05\n",
      "0xFF\n",
      "0x00\n",
      "0x03\n",
      "0x04\n",
      "0xFF\n",
      "0x00\n",
      "0x03\n",
      "0x04\n",
      "0x00\n",
      "0x01\n",
      "0x02\n",
      "0xFF\n",
      "0x00\n",
      "0xFF\n",
      "0x04\n",
      "0xFF\n",
      "0x00\n",
      "0x03\n",
      "0x03\n",
      "0x00\n",
      "0xFF\n",
      "0x02\n",
      "0x04\n",
      "0x01\n",
      "0xFE\n",
      "0x01\n",
      "0x02\n",
      "0x00\n",
      "0x01\n",
      "0x01\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0xFE\n",
      "0x00\n",
      "0xFF\n",
      "0xFE\n",
      "0xFE\n",
      "0x00\n",
      "0x00\n",
      "0xFD\n",
      "0x01\n",
      "0xFD\n",
      "0x02\n",
      "0x01\n",
      "0x01\n",
      "0xFE\n",
      "0xFE\n",
      "0xFE\n",
      "0x00\n",
      "0x00\n",
      "0x00\n",
      "0x01\n",
      "0x00\n",
      "0x01\n",
      "0xFF\n",
      "0xFF\n",
      "0x03\n",
      "0x05\n",
      "0x07\n",
      "0x03\n",
      "0xFD\n",
      "0x03\n",
      "0x00\n",
      "0xFF\n",
      "0x00\n",
      "0xFE\n",
      "0x00\n",
      "0xFF\n",
      "0xFE\n",
      "0xFE\n",
      "0x00\n",
      "0x01\n",
      "0x04\n",
      "0x00\n",
      "0xFE\n",
      "0xFD\n",
      "0xFD\n",
      "0xFE\n",
      "0xFD\n",
      "0xFE\n",
      "0xFE\n",
      "0xFD\n",
      "0xF7\n",
      "0xF6\n",
      "0xFE\n",
      "0x02\n",
      "0xFD\n",
      "0xFC\n",
      "0xFE\n",
      "0xFE\n",
      "0x02\n",
      "0x01\n",
      "0x03\n",
      "0xFE\n",
      "0x02\n",
      "0x03\n",
      "0x00\n",
      "0x00\n",
      "0x01\n",
      "0x03\n",
      "0xFF\n",
      "0xFD\n",
      "0xFB\n",
      "0x00\n",
      "0xFD\n",
      "0xFF\n",
      "0xFE\n",
      "0xFE\n",
      "0xFD\n",
      "0xFC\n",
      "0xFF\n",
      "0x01\n",
      "0x00\n",
      "0x00\n",
      "0xFF\n",
      "0xFE\n",
      "0x01\n",
      "0x00\n",
      "0x04\n",
      "0x03\n",
      "0xFD\n",
      "0xFA\n",
      "0xFE\n",
      "0x01\n",
      "0x02\n",
      "0xFD\n",
      "0xFF\n",
      "0x01\n",
      "0x04\n",
      "0xFE\n",
      "0xFF\n",
      "0xFD\n",
      "0xFF\n",
      "0x02\n",
      "0xFE\n",
      "0x03\n",
      "0x00\n",
      "0x04\n",
      "0x00\n",
      "0x02\n",
      "0xFF\n",
      "0x02\n",
      "0x02\n",
      "0xFF\n",
      "0xFF\n",
      "0x01\n",
      "0x03\n",
      "0x00\n",
      "0x02\n",
      "0x01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\3506092626.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv2_weight_11 =  torch.tensor((model.conv2.weight.data[0][0]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\3506092626.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv2_weight_12 =  torch.tensor((model.conv2.weight.data[1][0]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\3506092626.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv2_weight_13 =  torch.tensor((model.conv2.weight.data[2][0]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\3506092626.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv2_weight_21 =  torch.tensor((model.conv2.weight.data[3][0]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\3506092626.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv2_weight_22 =  torch.tensor((model.conv2.weight.data[4][0]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\3506092626.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv2_weight_23 =  torch.tensor((model.conv2.weight.data[5][0]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\3506092626.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv2_weight_31 =  torch.tensor((model.conv2.weight.data[6][0]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\3506092626.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv2_weight_32 =  torch.tensor((model.conv2.weight.data[7][0]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\3506092626.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv2_weight_33 =  torch.tensor((model.conv2.weight.data[8][0]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\3506092626.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv2_weight_41 =  torch.tensor((model.conv2.weight.data[9][0]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\3506092626.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv2_weight_42 =  torch.tensor((model.conv2.weight.data[10][0]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\3506092626.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv2_weight_43 =  torch.tensor((model.conv2.weight.data[11][0]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\3506092626.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv2_weight_51 =  torch.tensor((model.conv2.weight.data[12][0]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\3506092626.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv2_weight_52 =  torch.tensor((model.conv2.weight.data[13][0]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\3506092626.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv2_weight_53 =  torch.tensor((model.conv2.weight.data[14][0]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\3506092626.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv2_weight_61 =  torch.tensor((model.conv2.weight.data[15][0]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\3506092626.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv2_weight_62 =  torch.tensor((model.conv2.weight.data[16][0]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\3506092626.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv2_weight_63 =  torch.tensor((model.conv2.weight.data[17][0]), dtype = torch.float32)\n"
     ]
    }
   ],
   "source": [
    "############## Conv2 가중치 값 HEX 추출 ############\n",
    "print(np.shape(model.conv2.weight))\n",
    "\n",
    "# float => int\n",
    "\n",
    "float_conv2_weight_11 =  torch.tensor((model.conv2.weight.data[0][0]), dtype = torch.float32)\n",
    "float_conv2_weight_12 =  torch.tensor((model.conv2.weight.data[1][0]), dtype = torch.float32)\n",
    "float_conv2_weight_13 =  torch.tensor((model.conv2.weight.data[2][0]), dtype = torch.float32)\n",
    "\n",
    "float_conv2_weight_21 =  torch.tensor((model.conv2.weight.data[3][0]), dtype = torch.float32)\n",
    "float_conv2_weight_22 =  torch.tensor((model.conv2.weight.data[4][0]), dtype = torch.float32)\n",
    "float_conv2_weight_23 =  torch.tensor((model.conv2.weight.data[5][0]), dtype = torch.float32)\n",
    "\n",
    "float_conv2_weight_31 =  torch.tensor((model.conv2.weight.data[6][0]), dtype = torch.float32)\n",
    "float_conv2_weight_32 =  torch.tensor((model.conv2.weight.data[7][0]), dtype = torch.float32)\n",
    "float_conv2_weight_33 =  torch.tensor((model.conv2.weight.data[8][0]), dtype = torch.float32)\n",
    "\n",
    "float_conv2_weight_41 =  torch.tensor((model.conv2.weight.data[9][0]), dtype = torch.float32)\n",
    "float_conv2_weight_42 =  torch.tensor((model.conv2.weight.data[10][0]), dtype = torch.float32)\n",
    "float_conv2_weight_43 =  torch.tensor((model.conv2.weight.data[11][0]), dtype = torch.float32)\n",
    "\n",
    "float_conv2_weight_51 =  torch.tensor((model.conv2.weight.data[12][0]), dtype = torch.float32)\n",
    "float_conv2_weight_52 =  torch.tensor((model.conv2.weight.data[13][0]), dtype = torch.float32)\n",
    "float_conv2_weight_53 =  torch.tensor((model.conv2.weight.data[14][0]), dtype = torch.float32)\n",
    "\n",
    "float_conv2_weight_61 =  torch.tensor((model.conv2.weight.data[15][0]), dtype = torch.float32)\n",
    "float_conv2_weight_62 =  torch.tensor((model.conv2.weight.data[16][0]), dtype = torch.float32)\n",
    "float_conv2_weight_63 =  torch.tensor((model.conv2.weight.data[17][0]), dtype = torch.float32)\n",
    "\n",
    "\n",
    "#print (\"Signed\")\n",
    "print(float_conv2_weight_11)\n",
    "print(float_conv2_weight_12)\n",
    "print(float_conv2_weight_13)\n",
    "print(float_conv2_weight_21)\n",
    "print(float_conv2_weight_22)\n",
    "print(float_conv2_weight_23)\n",
    "print(float_conv2_weight_31)\n",
    "print(float_conv2_weight_32)\n",
    "print(float_conv2_weight_33)\n",
    "print(float_conv2_weight_41)\n",
    "print(float_conv2_weight_42)\n",
    "print(float_conv2_weight_43)\n",
    "print(float_conv2_weight_51)\n",
    "print(float_conv2_weight_52)\n",
    "print(float_conv2_weight_53)\n",
    "print(float_conv2_weight_61)\n",
    "print(float_conv2_weight_62)\n",
    "print(float_conv2_weight_63)\n",
    "\n",
    "float_conv2_weight = [[ 0.1184,  0.1777, -0.0042, -0.1301, -0.1472],\n",
    "        [-0.1608, -0.1476,  0.1198,  0.0973,  0.0907],\n",
    "        [ 0.1599, -0.0432,  0.1043, -0.1840,  0.0241],\n",
    "        [-0.1670, -0.0661, -0.0222, -0.1046, -0.0475],\n",
    "        [ 0.1665,  0.0124, -0.1940,  0.0043, -0.1785],\n",
    "[ 0.1317, -0.0243,  0.1257,  0.2123, -0.0665],\n",
    "        [-0.1901,  0.0699,  0.2054,  0.0943,  0.1152],\n",
    "        [-0.0420,  0.0330,  0.2671, -0.0268,  0.1289],\n",
    "        [ 0.1097,  0.1125,  0.0085,  0.1843, -0.1247],\n",
    "        [ 0.1298,  0.1431,  0.0524,  0.1489,  0.1840],\n",
    "[-0.2198,  0.0707,  0.2381,  0.0219, -0.1041],\n",
    "        [-0.1509,  0.1441,  0.1081,  0.1325, -0.1573],\n",
    "        [-0.0587, -0.0417,  0.2373,  0.0116, -0.0945],\n",
    "        [-0.0157, -0.0278, -0.0492, -0.0701,  0.1609],\n",
    "        [-0.0615, -0.0781, -0.0163,  0.2394,  0.1353],\n",
    "[-0.0962,  0.0199,  0.2266, -0.1618, -0.0050],\n",
    "        [-0.0727,  0.0810,  0.1946, -0.1478, -0.0307],\n",
    "        [ 0.0315,  0.1458, -0.0004, -0.0993, -0.1629],\n",
    "        [ 0.1195, -0.1430,  0.1795, -0.1107, -0.0704],\n",
    "        [-0.0971, -0.1640,  0.0645,  0.1642,  0.0202],\n",
    "[ 0.0505,  0.0796,  0.0385,  0.0586, -0.1782],\n",
    "        [-0.0611, -0.0952, -0.2513, -0.0485,  0.2246],\n",
    "        [-0.0773, -0.1652, -0.0665,  0.0073,  0.2135],\n",
    "        [-0.0557, -0.0851, -0.2560, -0.2050, -0.0694],\n",
    "        [ 0.1011, -0.1036,  0.1115,  0.0099, -0.1640],\n",
    "[ 0.1290, -0.1563,  0.1341,  0.1788, -0.0821],\n",
    "        [-0.1473, -0.1850,  0.1907,  0.1456,  0.1235],\n",
    "        [ 0.0419,  0.0074,  0.0123,  0.0296,  0.0577],\n",
    "        [-0.1207,  0.0259, -0.1164, -0.0834, -0.0293],\n",
    "        [ 0.0425, -0.1879, -0.1615, -0.0523,  0.1819],\n",
    "[-0.2999, -0.2512, -0.3695, -0.3287, -0.1825],\n",
    "        [ 0.0784,  0.1102,  0.0060, -0.1921, -0.0706],\n",
    "        [-0.0723,  0.3526,  0.2258,  0.2869,  0.2250],\n",
    "        [-0.0418,  0.0080, -0.1967, -0.2840,  0.1730],\n",
    "        [-0.0536, -0.0408,  0.0718, -0.1427,  0.0544],\n",
    "[-0.1292, -0.1552,  0.1568, -0.0277, -0.1359],\n",
    "        [-0.0848,  0.2838,  0.1847, -0.3486, -0.1957],\n",
    "        [ 0.1465,  0.1529, -0.2023, -0.2469,  0.2335],\n",
    "        [-0.1802,  0.1978, -0.0155, -0.1185,  0.0976],\n",
    "        [-0.0020, -0.0766,  0.2636, -0.0435,  0.3286],\n",
    "[ 0.1341, -0.2531, -0.3342, -0.2920, -0.1205],\n",
    "        [-0.2182, -0.0063, -0.0346, -0.2450, -0.1592],\n",
    "        [ 0.1975,  0.3989,  0.0252, -0.2345,  0.0569],\n",
    "        [ 0.1997, -0.1044, -0.0140, -0.1917,  0.0346],\n",
    "        [-0.0273, -0.1904, -0.1436,  0.1506,  0.0546],\n",
    "[ 0.1614, -0.0780,  0.1382,  0.1170, -0.0392],\n",
    "        [ 0.0955,  0.1066,  0.0379, -0.2111, -0.1429],\n",
    "        [ 0.1930,  0.0045, -0.3711, -0.2764, -0.2159],\n",
    "        [ 0.2218, -0.0204, -0.2184,  0.0313, -0.0597],\n",
    "        [ 0.1819,  0.0733,  0.1990, -0.1016,  0.2146],\n",
    "[ 0.0509,  0.2499,  0.2172,  0.0388,  0.1408],\n",
    "        [ 0.0590,  0.0518,  0.0766,  0.3883,  0.1145],\n",
    "        [-0.3542, -0.4204,  0.0768,  0.2111, -0.0452],\n",
    "        [-0.1748, -0.5332, -0.2188,  0.3843, -0.0563],\n",
    "        [-0.0901, -0.0735, -0.0298,  0.2214, -0.1340],\n",
    "[ 0.2469,  0.2628,  0.1577,  0.2869,  0.0428],\n",
    "        [-0.0219,  0.1019,  0.1583, -0.1550, -0.0962],\n",
    "        [-0.2320, -0.4619, -0.6237, -0.5795, -0.1699],\n",
    "        [-0.3245, -0.2049, -0.0837,  0.1662,  0.0400],\n",
    "        [-0.0076,  0.2074,  0.2341,  0.3290, -0.0857],\n",
    "[-0.0383,  0.2112,  0.3010, -0.0745,  0.0316],\n",
    "        [ 0.2390,  0.2874, -0.0581,  0.0664,  0.1628],\n",
    "        [-0.1148,  0.0006, -0.0919,  0.2606, -0.0914],\n",
    "        [-0.0016,  0.2269,  0.2143, -0.0500, -0.1143],\n",
    "        [ 0.1794,  0.3036,  0.1181, -0.1664,  0.1184],\n",
    "[ 0.1319,  0.0015,  0.1172,  0.0875,  0.0030],\n",
    "        [-0.0456,  0.0598, -0.1365, -0.0250, -0.1217],\n",
    "        [-0.1414, -0.1543,  0.0386, -0.0426, -0.2418],\n",
    "        [ 0.1072, -0.2245,  0.1625,  0.0802,  0.1022],\n",
    "        [-0.1416, -0.1283, -0.1646, -0.0346,  0.0324],\n",
    "[-0.0181,  0.1153, -0.0137,  0.1152, -0.0995],\n",
    "        [-0.1205,  0.2284,  0.3136,  0.4492,  0.2332],\n",
    "        [-0.1927,  0.2196, -0.0120, -0.0824,  0.0420],\n",
    "        [-0.1457,  0.0015, -0.1079, -0.1756, -0.1657],\n",
    "        [ 0.0151,  0.1027,  0.2652,  0.0101, -0.1275],\n",
    "[-0.2177, -0.2263, -0.1631, -0.2365, -0.1516],\n",
    "        [-0.1785, -0.2252, -0.5882, -0.6372, -0.1528],\n",
    "        [ 0.1287, -0.2361, -0.2780, -0.1335, -0.1741],\n",
    "        [ 0.1401,  0.0814,  0.2366, -0.1604,  0.1257],\n",
    "        [ 0.2126,  0.0126,  0.0569,  0.0815,  0.2374],\n",
    "[-0.0626, -0.1978, -0.3155,  0.0076, -0.2358],\n",
    "        [-0.1150, -0.1860, -0.1308, -0.2000, -0.2655],\n",
    "        [-0.1086,  0.0929,  0.0455, -0.0445, -0.0717],\n",
    "        [-0.1809,  0.0684,  0.0342,  0.2559,  0.2426],\n",
    "        [-0.2472, -0.3982, -0.1720,  0.1056,  0.1337],\n",
    "[-0.2339, -0.1244,  0.0774,  0.2545, -0.1562],\n",
    "        [-0.0712, -0.2176, -0.1240,  0.1255, -0.1296],\n",
    "        [ 0.2199, -0.0355,  0.2808, -0.0167,  0.1841],\n",
    "        [-0.0695,  0.1678,  0.1806, -0.0723, -0.0707],\n",
    "        [ 0.0850,  0.2393, -0.0052,  0.1778,  0.1152]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# signed int => unsigned int\n",
    "from fxpmath import Fxp\n",
    "for i in range(90):\n",
    "    for j in range(5):\n",
    "        \n",
    "        x = Fxp(float_conv2_weight[i][j], signed=True, n_word=8, n_frac=4)\n",
    "        #print(x)         # float value\n",
    "        #print(x.bin())   # binary representation\n",
    "        print(x.hex())\n",
    "        #print(x.val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 18, 1, 1])\n",
      "tensor([[0.1435]])\n",
      "tensor([[0.0478]])\n",
      "tensor([[0.2155]])\n",
      "tensor([[-0.0104]])\n",
      "tensor([[-0.0491]])\n",
      "tensor([[-0.0628]])\n",
      "tensor([[0.0053]])\n",
      "tensor([[0.3131]])\n",
      "tensor([[0.3038]])\n",
      "tensor([[-0.0861]])\n",
      "tensor([[0.0614]])\n",
      "tensor([[0.1344]])\n",
      "tensor([[-0.0841]])\n",
      "tensor([[0.0324]])\n",
      "tensor([[-0.1783]])\n",
      "tensor([[0.1439]])\n",
      "tensor([[0.1799]])\n",
      "tensor([[-0.1860]])\n",
      "0x02\n",
      "0x00\n",
      "0x03\n",
      "0x00\n",
      "0x00\n",
      "0xFF\n",
      "0x00\n",
      "0x05\n",
      "0x04\n",
      "0xFF\n",
      "0x00\n",
      "0x02\n",
      "0xFF\n",
      "0x00\n",
      "0xFE\n",
      "0x02\n",
      "0x02\n",
      "0xFE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\833655838.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv3_weight_11 =  torch.tensor((model.conv3.weight.data[0][0]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\833655838.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv3_weight_12 =  torch.tensor((model.conv3.weight.data[0][1]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\833655838.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv3_weight_13 =  torch.tensor((model.conv3.weight.data[0][2]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\833655838.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv3_weight_21 =  torch.tensor((model.conv3.weight.data[1][0]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\833655838.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv3_weight_22 =  torch.tensor((model.conv3.weight.data[1][1]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\833655838.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv3_weight_23 =  torch.tensor((model.conv3.weight.data[1][2]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\833655838.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv3_weight_31 =  torch.tensor((model.conv3.weight.data[2][0]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\833655838.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv3_weight_32 =  torch.tensor((model.conv3.weight.data[2][1]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\833655838.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv3_weight_33 =  torch.tensor((model.conv3.weight.data[2][2]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\833655838.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv3_weight_41 =  torch.tensor((model.conv3.weight.data[3][0]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\833655838.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv3_weight_42 =  torch.tensor((model.conv3.weight.data[3][1]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\833655838.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv3_weight_43 =  torch.tensor((model.conv3.weight.data[3][2]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\833655838.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv3_weight_51 =  torch.tensor((model.conv3.weight.data[4][0]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\833655838.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv3_weight_52 =  torch.tensor((model.conv3.weight.data[4][1]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\833655838.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv3_weight_53 =  torch.tensor((model.conv3.weight.data[4][2]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\833655838.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv3_weight_61 =  torch.tensor((model.conv3.weight.data[5][0]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\833655838.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv3_weight_62 =  torch.tensor((model.conv3.weight.data[5][1]), dtype = torch.float32)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33968\\833655838.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  float_conv3_weight_63 =  torch.tensor((model.conv3.weight.data[5][2]), dtype = torch.float32)\n"
     ]
    }
   ],
   "source": [
    "############## Conv3 가중치 값 HEX 추출 ############\n",
    "\n",
    "print(np.shape(model.conv3.weight))\n",
    "# float => int\n",
    "#float_conv3_bias = torch.tensor((model.conv3.bias.data), dtype = torch.float32)\n",
    "\n",
    "float_conv3_weight_11 =  torch.tensor((model.conv3.weight.data[0][0]), dtype = torch.float32)\n",
    "float_conv3_weight_12 =  torch.tensor((model.conv3.weight.data[0][1]), dtype = torch.float32)\n",
    "float_conv3_weight_13 =  torch.tensor((model.conv3.weight.data[0][2]), dtype = torch.float32)\n",
    "\n",
    "float_conv3_weight_21 =  torch.tensor((model.conv3.weight.data[1][0]), dtype = torch.float32)\n",
    "float_conv3_weight_22 =  torch.tensor((model.conv3.weight.data[1][1]), dtype = torch.float32)\n",
    "float_conv3_weight_23 =  torch.tensor((model.conv3.weight.data[1][2]), dtype = torch.float32)\n",
    "\n",
    "float_conv3_weight_31 =  torch.tensor((model.conv3.weight.data[2][0]), dtype = torch.float32)\n",
    "float_conv3_weight_32 =  torch.tensor((model.conv3.weight.data[2][1]), dtype = torch.float32)\n",
    "float_conv3_weight_33 =  torch.tensor((model.conv3.weight.data[2][2]), dtype = torch.float32)\n",
    "\n",
    "float_conv3_weight_41 =  torch.tensor((model.conv3.weight.data[3][0]), dtype = torch.float32)\n",
    "float_conv3_weight_42 =  torch.tensor((model.conv3.weight.data[3][1]), dtype = torch.float32)\n",
    "float_conv3_weight_43 =  torch.tensor((model.conv3.weight.data[3][2]), dtype = torch.float32)\n",
    "\n",
    "float_conv3_weight_51 =  torch.tensor((model.conv3.weight.data[4][0]), dtype = torch.float32)\n",
    "float_conv3_weight_52 =  torch.tensor((model.conv3.weight.data[4][1]), dtype = torch.float32)\n",
    "float_conv3_weight_53 =  torch.tensor((model.conv3.weight.data[4][2]), dtype = torch.float32)\n",
    "\n",
    "float_conv3_weight_61 =  torch.tensor((model.conv3.weight.data[5][0]), dtype = torch.float32)\n",
    "float_conv3_weight_62 =  torch.tensor((model.conv3.weight.data[5][1]), dtype = torch.float32)\n",
    "float_conv3_weight_63 =  torch.tensor((model.conv3.weight.data[5][2]), dtype = torch.float32)\n",
    "\n",
    "\n",
    "#print (\"Signed\")\n",
    "#print(float_conv3_bias)\n",
    "print(float_conv3_weight_11)\n",
    "print(float_conv3_weight_12)\n",
    "print(float_conv3_weight_13)\n",
    "print(float_conv3_weight_21)\n",
    "print(float_conv3_weight_22)\n",
    "print(float_conv3_weight_23)\n",
    "print(float_conv3_weight_31)\n",
    "print(float_conv3_weight_32)\n",
    "print(float_conv3_weight_33)\n",
    "print(float_conv3_weight_41)\n",
    "print(float_conv3_weight_42)\n",
    "print(float_conv3_weight_43)\n",
    "print(float_conv3_weight_51)\n",
    "print(float_conv3_weight_52)\n",
    "print(float_conv3_weight_53)\n",
    "print(float_conv3_weight_61)\n",
    "print(float_conv3_weight_62)\n",
    "print(float_conv3_weight_63)\n",
    "\n",
    "#float_conv3_bias = [[ 0.1263, -0.4903,  0.1191]]\n",
    "float_conv3_weight3 = [[0.1435],\n",
    "[0.0478],\n",
    "[0.2155],\n",
    "[-0.0104],\n",
    "[-0.0491],\n",
    "[-0.0628],\n",
    "[0.0053],\n",
    "[0.3131],\n",
    "[0.3038],\n",
    "[-0.0861],\n",
    "[0.0614],\n",
    "[0.1344],\n",
    "[-0.0841],\n",
    "[0.0324],\n",
    "[-0.1783],\n",
    "[0.1439],\n",
    "[0.1799],\n",
    "[-0.1860]]\n",
    "from fxpmath import Fxp\n",
    "\n",
    "#for m in range(1):\n",
    "#    for n in range(3):\n",
    "#        b = Fxp(float_conv3_bias[m][n], signed=True, n_word=16, n_frac=8\n",
    "   \n",
    "        #print(b)         # float value\n",
    "        #print(b.bin())   # binary representation\n",
    "#        print(b.hex())\n",
    "        #print(b.val)\n",
    "        \n",
    "\n",
    "\n",
    "for i in range(18):\n",
    "    for j in range(1):\n",
    "        \n",
    "        x = Fxp(float_conv3_weight3[i][j], signed=True, n_word=8, n_frac=4)\n",
    "        #print(x)         # float value\n",
    "        #print(x.bin())   # binary representation\n",
    "        print(x.hex())\n",
    "        #print(x.val)\n",
    "# signed int => unsigned int\n",
    "\n",
    "\n",
    "#for i in range(3):\n",
    "#    if int_conv3_bias[i] < 0:\n",
    "#            int_conv3_bias[i] += 256*256\n",
    "\n",
    "\n",
    "\n",
    "#np.savetxt('conv3_weight_11.txt', float_conv3_weight_11, fmt='%1.2x',delimiter = \" \")\n",
    "#np.savetxt('conv3_weight_12.txt', float_conv3_weight_12, fmt='%1.2x',delimiter = \" \")\n",
    "#np.savetxt('conv3_weight_13.txt', float_conv3_weight_13, fmt='%1.2x',delimiter = \" \")\n",
    "\n",
    "#np.savetxt('conv3_weight_21.txt', float_conv3_weight_21, fmt='%1.2x',delimiter = \" \")\n",
    "#np.savetxt('conv3_weight_22.txt', float_conv3_weight_22, fmt='%1.2x',delimiter = \" \")\n",
    "#np.savetxt('conv3_weight_23.txt', float_conv3_weight_23, fmt='%1.2x',delimiter = \" \")\n",
    "\n",
    "#np.savetxt('conv3_weight_31.txt', float_conv3_weight_31, fmt='%1.2x',delimiter = \" \")\n",
    "#np.savetxt('conv3_weight_32.txt', float_conv3_weight_32, fmt='%1.2x',delimiter = \" \")\n",
    "#np.savetxt('conv3_weight_33.txt', float_conv3_weight_33, fmt='%1.2x',delimiter = \" \")\n",
    "\n",
    "#np.savetxt('conv3_bias.txt', int_conv3_bias, fmt='%1.2x',delimiter = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-0.1474,  0.0831, -0.0449, -0.1405,  0.0600,  0.0787, -0.0340, -0.0387,\n",
      "        -0.1353,  0.0976], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-6.6225e-02,  3.2556e-02,  5.5242e-02,  6.8833e-02,  8.6716e-02,\n",
      "          1.4086e-01,  8.5182e-02,  1.1414e-01, -3.7614e-02, -2.3148e-02,\n",
      "         -9.5934e-02,  7.5057e-02, -4.4981e-02,  1.3153e-01, -1.3041e-01,\n",
      "         -1.1367e-01, -9.4842e-03, -5.3155e-02,  8.4044e-02, -2.3950e-02,\n",
      "         -1.0267e-01, -6.4267e-02,  1.5496e-01,  6.7653e-02, -3.3393e-01,\n",
      "         -2.6816e-01, -1.0517e-01, -7.7368e-02, -6.3888e-02, -3.5167e-02,\n",
      "         -3.7750e-02,  1.2781e-01,  3.3492e-02, -3.4174e-02,  5.9913e-02,\n",
      "          1.1882e-01,  1.5604e-02,  9.1628e-02, -1.2629e-02,  2.2870e-01,\n",
      "          9.1511e-02, -1.4586e-01,  8.3657e-02,  2.3657e-01,  2.4047e-01,\n",
      "         -1.1409e-01, -1.7323e-01,  4.7743e-03, -2.0424e-02,  4.9587e-02,\n",
      "          4.1078e-02,  2.3212e-01,  1.3312e-01,  1.3742e-01, -4.9071e-02,\n",
      "          6.2087e-02, -5.6849e-02, -1.0033e-01, -2.2385e-01, -3.1123e-01,\n",
      "         -1.0135e-01,  1.0084e-01,  2.0027e-01, -3.5494e-02,  1.0980e-02,\n",
      "          1.3104e-01,  6.6422e-02, -7.0186e-02,  7.2191e-02,  3.9484e-02,\n",
      "          8.4425e-02, -8.9432e-02, -8.4961e-02, -8.4606e-02,  5.5238e-02,\n",
      "          4.5593e-02, -1.6321e-01,  8.2383e-02, -1.0513e-02,  8.7935e-02,\n",
      "         -5.5364e-02,  1.0673e-01, -7.1496e-02, -7.9954e-02,  4.6827e-02,\n",
      "         -7.9740e-02, -2.3537e-01, -1.1592e-01,  8.0778e-02, -2.6015e-01,\n",
      "          5.3951e-02,  2.2513e-01,  8.5862e-02, -3.2028e-02,  1.8300e-01,\n",
      "          5.5398e-02],\n",
      "        [-8.4095e-02,  6.3263e-02,  1.6329e-01,  2.3265e-02, -5.9549e-03,\n",
      "         -6.2045e-02,  1.7890e-02,  7.8671e-02,  1.1074e-01,  2.8243e-03,\n",
      "         -3.2255e-02, -2.5943e-02, -4.5984e-02,  1.0443e-01,  2.6870e-02,\n",
      "         -8.8618e-03, -2.6972e-02, -1.6184e-01, -2.5233e-01, -2.8135e-01,\n",
      "         -6.0017e-02,  1.0854e-01, -2.7072e-01, -1.6214e-01, -1.5560e-01,\n",
      "         -5.0585e-02, -6.2610e-02, -4.6266e-03, -2.3314e-01,  2.1398e-02,\n",
      "          4.2059e-02,  5.1598e-02, -2.6564e-02,  1.6243e-01,  2.5057e-01,\n",
      "          5.6381e-02, -1.0354e-01,  8.7524e-02,  1.4860e-01, -1.2677e-01,\n",
      "         -9.9082e-02,  2.5347e-01,  3.8864e-02,  5.7119e-02, -1.0164e-01,\n",
      "          1.1912e-01,  1.0576e-01,  5.5529e-02, -1.0382e-02, -1.5697e-01,\n",
      "          9.9296e-02,  1.7580e-02,  1.1084e-01,  5.7855e-02,  2.1289e-01,\n",
      "         -9.0302e-02, -4.7572e-02,  7.4237e-02,  2.4430e-01, -5.2952e-02,\n",
      "          1.6876e-01,  8.0620e-02, -7.3173e-02,  2.7215e-01,  9.4167e-02,\n",
      "         -1.6603e-03, -1.4635e-01,  1.2108e-02, -1.2075e-01,  1.3089e-01,\n",
      "          1.5438e-01, -1.7972e-02,  9.9362e-02, -1.2370e-01, -3.5695e-02,\n",
      "         -1.7482e-03,  1.4912e-01,  8.5569e-02,  1.9617e-01,  1.7401e-01,\n",
      "          2.8788e-02, -4.5917e-03,  1.1330e-01,  8.4029e-02, -8.7452e-02,\n",
      "          2.5682e-01, -2.5958e-01, -3.0242e-01,  5.5701e-02, -2.7349e-02,\n",
      "         -1.2907e-01, -2.9114e-01,  7.3984e-02,  1.5232e-01, -4.9229e-01,\n",
      "          1.2103e-01],\n",
      "        [ 3.7664e-02, -1.2190e-02,  9.6625e-02,  6.2210e-02, -1.1727e-01,\n",
      "         -1.1562e-01,  2.0475e-01,  4.3782e-02, -1.2909e-01, -2.8440e-02,\n",
      "         -2.9444e-02, -1.9192e-01,  1.9533e-02, -6.8360e-03, -3.4114e-02,\n",
      "          2.5344e-02,  9.4265e-02,  1.0698e-01,  1.0703e-01,  6.3482e-03,\n",
      "          4.3164e-01,  1.0142e-01,  4.1497e-02,  2.1196e-01, -5.6719e-02,\n",
      "         -9.3606e-02, -8.5046e-02, -1.7235e-01, -2.9160e-01,  4.7828e-02,\n",
      "         -2.5945e-02,  2.4157e-01, -1.2213e-01, -2.0752e-02,  9.7038e-02,\n",
      "          3.4821e-02, -1.8796e-01, -2.0718e-01,  9.7396e-02,  1.1684e-01,\n",
      "         -9.9320e-02, -4.9574e-02,  1.5355e-01, -3.1482e-01,  1.9748e-01,\n",
      "         -8.9494e-02, -1.3103e-01,  2.0197e-03, -1.8131e-01, -3.5504e-03,\n",
      "          1.3379e-01,  5.5091e-02,  1.3466e-01,  1.2647e-01,  7.1849e-02,\n",
      "         -2.4761e-02,  3.7153e-01,  3.8314e-02, -6.3391e-02, -1.3208e-01,\n",
      "         -1.1597e-01, -8.6217e-02,  1.3040e-02,  3.4986e-01,  2.1997e-02,\n",
      "         -6.1772e-02,  3.9376e-02, -3.2735e-01,  2.1495e-01, -3.5137e-02,\n",
      "         -1.6507e-01, -1.1525e-02,  4.4876e-02,  1.1829e-02, -8.2609e-03,\n",
      "          2.7066e-01,  5.8643e-02, -4.9348e-02, -2.5994e-02,  3.0485e-01,\n",
      "          8.7909e-03, -4.9846e-02, -1.5638e-01, -2.8386e-01, -2.4185e-02,\n",
      "         -1.6744e-02, -1.3982e-03, -2.8604e-02,  2.1444e-01,  1.0324e-01,\n",
      "          5.0863e-02, -7.6575e-02,  2.4376e-01,  8.4093e-02, -1.6324e-01,\n",
      "          2.2216e-01],\n",
      "        [-4.0470e-02,  3.2170e-02,  7.3006e-02,  8.6206e-02,  4.1420e-02,\n",
      "         -3.8547e-02,  2.4198e-02, -8.4722e-02,  1.0197e-02,  8.7263e-02,\n",
      "         -1.2745e-01, -1.9789e-04,  2.6842e-02,  1.3672e-02,  8.5757e-02,\n",
      "         -6.1096e-02,  2.2066e-01,  2.5827e-01, -6.0008e-03, -1.2356e-01,\n",
      "          3.6031e-01,  3.2153e-02,  3.0802e-02, -1.8758e-01, -3.6237e-02,\n",
      "          3.1618e-02,  8.6423e-02,  2.3901e-02,  2.2253e-01,  1.2660e-01,\n",
      "         -2.3803e-02, -1.3003e-01, -3.4190e-01, -2.2100e-01,  4.4635e-02,\n",
      "          1.7485e-01, -5.4655e-02,  2.0100e-01, -1.2380e-01, -2.6847e-01,\n",
      "         -1.0482e-02,  1.7076e-01, -2.6683e-01, -5.2096e-02, -2.9916e-01,\n",
      "         -2.7215e-01, -1.9875e-02, -2.2110e-02, -9.6906e-02, -5.0813e-02,\n",
      "         -3.0514e-03, -1.4289e-01, -1.2275e-01, -2.1364e-01, -1.5910e-01,\n",
      "         -1.8805e-01, -9.3259e-03, -1.4689e-01, -1.8086e-01,  1.6982e-01,\n",
      "          1.9228e-01,  1.1483e-01, -1.0245e-01, -5.3320e-02,  3.1561e-02,\n",
      "          3.0372e-02,  1.0566e-01, -9.4280e-02,  4.8850e-02, -1.8293e-02,\n",
      "          7.4666e-02, -1.5537e-01,  1.1456e-01,  1.4605e-01,  5.2352e-03,\n",
      "         -9.2179e-02,  1.3659e-01, -2.5743e-02, -1.1507e-01, -2.2098e-01,\n",
      "         -1.0834e-01,  1.0111e-01,  7.5301e-02, -3.6238e-02,  4.9994e-02,\n",
      "          2.3608e-01,  3.3682e-01,  2.2014e-01, -1.3645e-01, -1.1892e-01,\n",
      "          5.4488e-02, -3.4124e-02, -1.6638e-01,  6.7973e-02,  3.8814e-01,\n",
      "          3.3219e-02],\n",
      "        [ 1.1308e-01,  8.2292e-02,  1.9947e-02,  7.4375e-02, -2.9953e-02,\n",
      "         -1.1148e-01, -1.7058e-01, -1.5541e-01, -3.2702e-02, -5.6337e-02,\n",
      "         -7.1867e-02, -3.5793e-02, -1.3938e-02, -2.5947e-02, -3.7676e-02,\n",
      "          1.5220e-02,  2.7737e-02, -3.2479e-01, -3.6800e-01, -1.6443e-01,\n",
      "         -1.9411e-01, -2.9360e-01, -2.3616e-01, -1.7911e-01,  9.9358e-02,\n",
      "          7.1145e-02,  4.3900e-02,  5.9845e-02,  2.1982e-01,  1.0244e-01,\n",
      "          1.1474e-01,  9.6551e-02,  2.4892e-01,  1.5009e-01,  3.1459e-01,\n",
      "          2.7056e-01,  1.2293e-01,  5.0492e-02,  2.0291e-01,  1.6220e-01,\n",
      "         -1.1057e-01, -3.9266e-02,  4.2088e-02, -1.4810e-01, -2.9828e-02,\n",
      "          2.0666e-01,  8.4155e-02,  2.3866e-02,  1.1577e-02,  1.3587e-01,\n",
      "         -1.5831e-02, -7.2568e-03,  8.8296e-03,  1.1342e-01, -1.7735e-01,\n",
      "         -2.4566e-03, -2.8070e-02,  8.8844e-02,  2.1838e-01,  2.2268e-01,\n",
      "          5.8904e-03,  8.9845e-02, -8.0377e-02, -2.2781e-01, -1.3939e-01,\n",
      "         -2.7587e-01, -2.7923e-01, -1.6532e-01, -8.0520e-02, -3.0224e-02,\n",
      "          3.9469e-03, -2.8205e-02,  1.9692e-02,  6.3267e-02,  8.2390e-02,\n",
      "         -4.6643e-02,  2.7274e-02, -1.2639e-01,  7.1896e-02,  4.0444e-02,\n",
      "         -5.0740e-02, -7.1817e-02,  2.0229e-01,  1.3291e-01,  1.9402e-01,\n",
      "         -3.9710e-02,  6.7428e-02, -3.2232e-02, -1.3801e-01, -1.6778e-01,\n",
      "          1.5196e-01, -5.1015e-02, -1.1681e-01, -9.7400e-02, -3.2690e-01,\n",
      "         -1.8869e-02],\n",
      "        [ 7.0306e-02,  6.1576e-02, -1.8232e-01, -1.7878e-01,  1.3253e-02,\n",
      "          2.3962e-02, -1.3051e-01, -4.0697e-02, -6.2435e-02, -7.1635e-03,\n",
      "          2.7667e-02,  1.9461e-01,  2.0451e-02,  8.9417e-02,  1.1847e-01,\n",
      "         -5.3932e-02, -1.2022e-01,  1.1547e-02,  1.9381e-01,  1.5139e-01,\n",
      "         -5.4729e-02,  7.7917e-02, -7.4178e-02,  2.0149e-01,  1.4637e-01,\n",
      "          8.5925e-02,  1.2594e-01,  6.4392e-02,  3.2194e-01, -1.5965e-02,\n",
      "          8.4646e-02, -1.0490e-01,  7.9911e-02,  6.7037e-03, -2.9646e-01,\n",
      "         -4.2846e-01,  2.2150e-01,  1.5781e-02, -3.7039e-01, -4.2994e-01,\n",
      "         -3.0347e-02, -1.1471e-01,  3.8932e-02,  8.9337e-02, -9.4180e-02,\n",
      "         -1.2295e-01, -6.8640e-02,  1.6956e-01, -1.3404e-02,  2.3860e-02,\n",
      "         -2.0042e-02, -1.9561e-01, -9.8455e-02, -2.1969e-02,  1.0084e-01,\n",
      "          9.7036e-02,  1.4615e-01,  4.7965e-02,  6.6582e-02,  8.1162e-02,\n",
      "          1.6618e-01, -8.3714e-02, -4.5869e-02,  6.1326e-02, -9.2739e-02,\n",
      "          8.3452e-02,  2.3759e-01,  4.0640e-01, -3.6018e-02, -6.3108e-02,\n",
      "          1.3316e-02,  1.6075e-01,  9.0032e-02,  1.1810e-03, -6.5371e-02,\n",
      "         -1.0350e-01,  3.0301e-01,  5.2208e-02, -1.5181e-01, -1.1825e-01,\n",
      "         -1.2844e-01, -9.7420e-02, -2.0221e-02,  1.9430e-01, -1.3931e-01,\n",
      "         -1.4453e-02, -2.1196e-01, -2.1577e-01, -2.5744e-02, -1.5737e-01,\n",
      "         -9.2890e-02, -2.2314e-01, -4.0968e-02,  1.8771e-01,  2.2581e-01,\n",
      "          5.7778e-02],\n",
      "        [-4.7018e-02,  1.4302e-01, -4.4327e-02, -1.3050e-01, -6.3246e-02,\n",
      "         -3.3748e-02, -2.9534e-02, -2.7970e-02,  1.0837e-01,  1.5011e-02,\n",
      "         -4.6480e-02,  4.0015e-02, -5.1142e-02,  6.3716e-03,  8.8488e-03,\n",
      "         -1.2526e-02, -2.4487e-01, -3.3652e-01, -2.1464e-01,  3.4777e-01,\n",
      "         -2.4318e-01, -2.8408e-01,  7.7136e-03,  2.7977e-01, -3.0212e-01,\n",
      "         -1.0295e-01, -4.7130e-02, -1.9075e-01,  4.6135e-02, -1.4775e-01,\n",
      "          9.5834e-02, -1.9962e-03,  1.2141e-01,  3.9361e-02,  7.2808e-02,\n",
      "         -2.5940e-01,  9.2774e-03,  1.8890e-02, -2.4703e-01, -1.3669e-01,\n",
      "          1.0613e-01,  6.1018e-02, -1.3803e-01,  8.9025e-02,  3.1226e-01,\n",
      "          1.0175e-01, -9.8680e-02, -6.1244e-02, -5.2437e-02, -1.4456e-01,\n",
      "         -1.3720e-01, -1.6016e-01, -4.5704e-02,  4.8326e-02,  1.4195e-01,\n",
      "          2.6608e-01, -9.2636e-02,  1.4359e-01,  2.8097e-02, -4.2358e-02,\n",
      "         -2.0102e-01, -1.3536e-01,  1.0053e-01,  1.7321e-02, -6.5864e-02,\n",
      "         -1.5057e-01,  7.4170e-02,  6.1726e-02, -5.4822e-02, -4.1805e-02,\n",
      "          2.0091e-01,  3.2085e-01, -1.5857e-01, -3.0637e-02,  2.7448e-02,\n",
      "         -3.8163e-02, -1.8654e-01,  7.7815e-02,  1.9332e-02, -4.6774e-02,\n",
      "          5.4963e-03, -7.2700e-03, -4.6082e-02, -1.3000e-02,  1.7182e-01,\n",
      "         -2.3271e-01, -2.5716e-01, -1.0892e-01,  6.2360e-02,  2.8512e-01,\n",
      "         -1.5542e-01,  3.2691e-01,  3.1535e-02,  3.9158e-02,  3.5225e-01,\n",
      "          3.0335e-01],\n",
      "        [ 2.0869e-02,  1.3971e-01,  1.2722e-01,  5.9499e-02,  7.9136e-02,\n",
      "          3.2876e-02,  6.3345e-02, -1.0381e-01,  7.0327e-02, -5.6021e-02,\n",
      "          4.7694e-02, -1.6968e-01,  3.7234e-02,  5.0338e-02, -1.8386e-02,\n",
      "         -1.2451e-01,  2.1683e-01,  1.2961e-01,  5.9455e-02, -5.5498e-02,\n",
      "          2.8876e-01,  1.4261e-01,  1.8487e-01, -9.5894e-02,  2.4031e-01,\n",
      "         -8.3247e-02, -1.6296e-02,  5.4169e-02, -2.9342e-02, -2.6003e-01,\n",
      "          2.8941e-02,  1.6257e-01, -5.3330e-02, -4.9601e-02, -1.4858e-03,\n",
      "         -1.3697e-01, -1.5934e-01, -7.5391e-02,  3.2852e-01,  1.2177e-01,\n",
      "          8.7232e-02,  9.4942e-02,  2.7874e-01,  1.3071e-01, -1.8636e-01,\n",
      "          2.8442e-01,  1.0103e-01, -1.9614e-01,  7.8748e-02,  1.8730e-01,\n",
      "          1.2427e-01,  1.6539e-01, -1.5243e-02,  2.1468e-01,  2.8279e-02,\n",
      "         -6.2101e-02, -1.3047e-01, -1.0014e-01,  1.8369e-01,  1.3831e-01,\n",
      "         -5.8661e-02,  1.1693e-01, -9.1601e-02, -3.0088e-01, -9.1717e-03,\n",
      "          1.1009e-01,  7.9640e-02,  1.3871e-01,  7.3086e-04, -1.0700e-01,\n",
      "          1.7048e-01,  5.9855e-02,  2.8726e-01,  1.2809e-01, -1.5109e-01,\n",
      "          7.2226e-02, -8.8942e-02,  1.4850e-02,  1.6172e-02, -7.1029e-03,\n",
      "         -2.4776e-01, -2.4779e-01, -1.2215e-01, -4.2312e-02, -1.3449e-01,\n",
      "          6.6630e-02, -1.1936e-01,  3.1432e-02,  1.8863e-01, -1.8296e-01,\n",
      "         -1.1521e-01, -2.9818e-03, -8.0898e-02, -9.3964e-02, -4.4481e-01,\n",
      "         -3.7265e-01],\n",
      "        [-1.0915e-01,  6.0487e-03, -2.6825e-01,  6.0598e-02, -2.4549e-02,\n",
      "          8.8001e-02, -2.4926e-02, -1.2354e-01,  3.0695e-02,  7.9460e-02,\n",
      "         -9.7738e-02, -3.3442e-02, -5.9938e-02,  7.3952e-02, -2.8283e-02,\n",
      "          2.1566e-01, -8.4645e-02,  1.7345e-01,  2.8290e-02,  7.3085e-02,\n",
      "         -7.0060e-02, -5.4586e-02, -7.1739e-02, -9.9966e-02,  1.2217e-01,\n",
      "          2.4147e-01, -8.9706e-02,  2.0175e-01, -3.4084e-01,  1.6492e-01,\n",
      "         -2.5207e-02, -1.4302e-01,  1.4863e-01,  1.0918e-01, -1.1954e-01,\n",
      "         -2.8527e-02, -5.9190e-03, -1.1985e-01, -2.6616e-01,  3.9518e-02,\n",
      "         -4.7514e-02,  3.1681e-03, -1.4639e-01, -1.1669e-01,  2.8215e-01,\n",
      "         -8.4671e-02, -1.5599e-01, -1.0251e-02,  4.0808e-02, -1.6813e-01,\n",
      "         -1.3070e-01,  1.0655e-02,  5.2427e-02, -2.2039e-01,  8.3956e-02,\n",
      "         -1.7240e-01,  9.6387e-02, -3.7873e-02,  5.4630e-02, -2.1626e-02,\n",
      "         -1.2024e-01, -3.7603e-01, -6.9198e-02,  1.9696e-02,  7.6373e-02,\n",
      "          8.2508e-02,  1.3725e-01, -2.6848e-02, -2.6020e-01, -6.3209e-02,\n",
      "         -5.6235e-02, -1.9267e-02, -1.3467e-01, -7.5441e-02,  2.9640e-02,\n",
      "         -1.5758e-02, -3.4012e-02, -2.0289e-01, -1.5440e-01, -1.5100e-01,\n",
      "          2.7721e-01,  5.4123e-03,  1.4862e-01,  1.6594e-01, -4.2599e-02,\n",
      "         -6.9132e-02,  2.1144e-01,  3.0043e-01,  1.2972e-01,  3.3903e-01,\n",
      "         -1.0348e-01,  6.1745e-03,  6.2743e-02, -1.0543e-01,  2.4746e-01,\n",
      "          5.3212e-02],\n",
      "        [-1.5756e-01, -1.5974e-01, -1.0484e-01,  1.4071e-01, -1.0550e-01,\n",
      "         -3.1597e-02,  8.8742e-02,  1.6610e-01, -4.4592e-02,  2.2722e-02,\n",
      "         -5.5368e-02,  8.3352e-02,  6.4819e-02, -3.4194e-02, -1.5154e-02,\n",
      "         -6.0946e-02, -5.5572e-02,  2.5791e-02,  2.1878e-01,  1.1456e-02,\n",
      "         -3.1748e-01, -1.5705e-01,  8.7747e-04,  9.1801e-02,  8.3374e-02,\n",
      "          1.9374e-01,  1.0021e-01, -2.8527e-02,  3.2867e-01,  1.5340e-01,\n",
      "         -1.1741e-01, -2.0426e-01, -8.9856e-04, -1.6089e-01, -2.9962e-01,\n",
      "         -1.1050e-01,  6.5009e-02, -1.3030e-02, -4.9626e-02,  1.8599e-01,\n",
      "         -3.2962e-03, -6.2939e-02,  1.3340e-01,  1.0585e-01, -1.4301e-01,\n",
      "          5.3546e-02,  1.1987e-01,  2.4238e-01,  3.1716e-02, -1.2747e-01,\n",
      "          1.5714e-02,  1.1639e-01, -1.3231e-01, -3.2741e-01, -4.6124e-02,\n",
      "          1.1899e-01, -2.0949e-01,  1.3598e-01, -2.0763e-01, -7.2700e-02,\n",
      "         -3.2497e-03,  1.6921e-01, -1.2114e-01, -3.4188e-01,  1.2861e-01,\n",
      "          1.5374e-01,  3.4947e-02,  6.9252e-03,  4.5859e-02,  1.0792e-03,\n",
      "         -2.4421e-01, -3.7357e-01, -2.2682e-02,  9.4811e-03,  6.7646e-02,\n",
      "         -1.7351e-01, -7.8633e-02,  9.0672e-02,  1.6199e-01,  3.8564e-03,\n",
      "          8.4979e-02,  2.3677e-01,  5.5735e-02, -8.2782e-02, -1.2861e-01,\n",
      "         -1.8049e-01,  3.5351e-01,  8.1959e-02, -4.8443e-01,  1.7036e-01,\n",
      "          1.7495e-01,  2.1059e-02, -1.4918e-01, -5.4696e-03, -2.8341e-01,\n",
      "         -3.7555e-01]], requires_grad=True)\n",
      "0xFFD8\n",
      "0x001E\n",
      "0xFFF1\n",
      "0xFFFD\n",
      "0xFFCA\n",
      "0x003E\n",
      "0xFFDF\n",
      "0x002D\n",
      "0xFFE3\n",
      "0xFFFB\n",
      "0x00\n",
      "0x01\n",
      "0x03\n",
      "0xFF\n",
      "0x01\n",
      "0x00\n",
      "0xFF\n",
      "0x00\n",
      "0x01\n",
      "0x00\n",
      "0xFE\n",
      "0x03\n",
      "0x03\n",
      "0x01\n",
      "0x00\n",
      "0x03\n",
      "0xFF\n",
      "0x01\n",
      "0x01\n",
      "0x01\n",
      "0x01\n",
      "0xFC\n",
      "0x00\n",
      "0x02\n",
      "0x00\n",
      "0xFF\n",
      "0xFE\n",
      "0xFF\n",
      "0x04\n",
      "0x00\n",
      "0x00\n",
      "0xFF\n",
      "0x00\n",
      "0x02\n",
      "0x00\n",
      "0x00\n",
      "0x02\n",
      "0xFF\n",
      "0xFF\n",
      "0x01\n",
      "0xFE\n",
      "0xFA\n",
      "0xFF\n",
      "0x02\n",
      "0x02\n",
      "0x00\n",
      "0x00\n",
      "0xFE\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\OneDrive - 광운대학교\\Capstone\\CNN\\my\\lenet5_336\\MNIST_CNN.ipynb 셀 6\u001b[0m in \u001b[0;36m<cell line: 83>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20%EA%B4%91%EC%9A%B4%EB%8C%80%ED%95%99%EA%B5%90/Capstone/CNN/my/lenet5_336/MNIST_CNN.ipynb#W5sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20%EA%B4%91%EC%9A%B4%EB%8C%80%ED%95%99%EA%B5%90/Capstone/CNN/my/lenet5_336/MNIST_CNN.ipynb#W5sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m96\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20%EA%B4%91%EC%9A%B4%EB%8C%80%ED%95%99%EA%B5%90/Capstone/CNN/my/lenet5_336/MNIST_CNN.ipynb#W5sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m         x \u001b[39m=\u001b[39m Fxp(fc_weight[i][j], signed\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, n_word\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, n_frac\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20%EA%B4%91%EC%9A%B4%EB%8C%80%ED%95%99%EA%B5%90/Capstone/CNN/my/lenet5_336/MNIST_CNN.ipynb#W5sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m         \u001b[39m#print(x)         # float value\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20%EA%B4%91%EC%9A%B4%EB%8C%80%ED%95%99%EA%B5%90/Capstone/CNN/my/lenet5_336/MNIST_CNN.ipynb#W5sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m         \u001b[39m#print(x.bin())   # binary representation\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20%EA%B4%91%EC%9A%B4%EB%8C%80%ED%95%99%EA%B5%90/Capstone/CNN/my/lenet5_336/MNIST_CNN.ipynb#W5sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m         \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mhex())\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "############## FC Layer가중치 값 HEX 추출 ############\n",
    "\n",
    "##(np.shape(model.fc_1.weight))\n",
    "#print((model.fc_1.weight * 128).int())\n",
    "\n",
    "#print(np.shape(model.fc_1.bias))\n",
    "\n",
    "\n",
    "float_fc_weight = (model.fc_1.weight).float()\n",
    "float_fc_bias = (model.fc_1.bias).float()\n",
    "print(float_fc_bias)\n",
    "print(float_fc_weight)\n",
    "fc_weight = [[-6.6225e-02,  3.2556e-02,  5.5242e-02,  6.8833e-02,  8.6716e-02,\n",
    "          1.4086e-01,  8.5182e-02,  1.1414e-01, -3.7614e-02, -2.3148e-02,\n",
    "         -9.5934e-02,  7.5057e-02, -4.4981e-02,  1.3153e-01, -1.3041e-01,\n",
    "         -1.1367e-01, -9.4842e-03, -5.3155e-02,  8.4044e-02, -2.3950e-02,\n",
    "         -1.0267e-01, -6.4267e-02,  1.5496e-01,  6.7653e-02, -3.3393e-01,\n",
    "         -2.6816e-01, -1.0517e-01, -7.7368e-02, -6.3888e-02, -3.5167e-02,\n",
    "         -3.7750e-02,  1.2781e-01,  3.3492e-02, -3.4174e-02,  5.9913e-02,\n",
    "          1.1882e-01,  1.5604e-02,  9.1628e-02, -1.2629e-02,  2.2870e-01,\n",
    "          9.1511e-02, -1.4586e-01,  8.3657e-02,  2.3657e-01,  2.4047e-01,\n",
    "         -1.1409e-01, -1.7323e-01,  4.7743e-03, -2.0424e-02,  4.9587e-02,\n",
    "          4.1078e-02,  2.3212e-01,  1.3312e-01,  1.3742e-01, -4.9071e-02,\n",
    "          6.2087e-02, -5.6849e-02, -1.0033e-01, -2.2385e-01, -3.1123e-01,\n",
    "         -1.0135e-01,  1.0084e-01,  2.0027e-01, -3.5494e-02,  1.0980e-02,\n",
    "          1.3104e-01,  6.6422e-02, -7.0186e-02,  7.2191e-02,  3.9484e-02,\n",
    "          8.4425e-02, -8.9432e-02, -8.4961e-02, -8.4606e-02,  5.5238e-02,\n",
    "          4.5593e-02, -1.6321e-01,  8.2383e-02, -1.0513e-02,  8.7935e-02,\n",
    "         -5.5364e-02,  1.0673e-01, -7.1496e-02, -7.9954e-02,  4.6827e-02,\n",
    "         -7.9740e-02, -2.3537e-01, -1.1592e-01,  8.0778e-02, -2.6015e-01,\n",
    "          5.3951e-02,  2.2513e-01,  8.5862e-02, -3.2028e-02,  1.8300e-01,\n",
    "          5.5398e-02],\n",
    "        [-8.4095e-02,  6.3263e-02,  1.6329e-01,  2.3265e-02, -5.9549e-03,\n",
    "         -6.2045e-02,  1.7890e-02,  7.8671e-02,  1.1074e-01,  2.8243e-03,\n",
    "         -3.2255e-02, -2.5943e-02, -4.5984e-02,  1.0443e-01,  2.6870e-02,\n",
    "         -8.8618e-03, -2.6972e-02, -1.6184e-01, -2.5233e-01, -2.8135e-01,\n",
    "         -6.0017e-02,  1.0854e-01, -2.7072e-01, -1.6214e-01, -1.5560e-01,\n",
    "         -5.0585e-02, -6.2610e-02, -4.6266e-03, -2.3314e-01,  2.1398e-02,\n",
    "          4.2059e-02,  5.1598e-02, -2.6564e-02,  1.6243e-01,  2.5057e-01,\n",
    "          5.6381e-02, -1.0354e-01,  8.7524e-02,  1.4860e-01, -1.2677e-01,\n",
    "         -9.9082e-02,  2.5347e-01,  3.8864e-02,  5.7119e-02, -1.0164e-01,\n",
    "          1.1912e-01,  1.0576e-01,  5.5529e-02, -1.0382e-02, -1.5697e-01,\n",
    "          9.9296e-02,  1.7580e-02,  1.1084e-01,  5.7855e-02,  2.1289e-01,\n",
    "         -9.0302e-02, -4.7572e-02,  7.4237e-02,  2.4430e-01, -5.2952e-02,\n",
    "          1.6876e-01,  8.0620e-02, -7.3173e-02,  2.7215e-01,  9.4167e-02,\n",
    "         -1.6603e-03, -1.4635e-01,  1.2108e-02, -1.2075e-01,  1.3089e-01,\n",
    "          1.5438e-01, -1.7972e-02,  9.9362e-02, -1.2370e-01, -3.5695e-02,\n",
    "         -1.7482e-03,  1.4912e-01,  8.5569e-02,  1.9617e-01,  1.7401e-01,\n",
    "          2.8788e-02, -4.5917e-03,  1.1330e-01,  8.4029e-02, -8.7452e-02,\n",
    "          2.5682e-01, -2.5958e-01, -3.0242e-01,  5.5701e-02, -2.7349e-02,\n",
    "         -1.2907e-01, -2.9114e-01,  7.3984e-02,  1.5232e-01, -4.9229e-01,\n",
    "          1.2103e-01],\n",
    "        [ 3.7664e-02, -1.2190e-02,  9.6625e-02,  6.2210e-02, -1.1727e-01,\n",
    "         -1.1562e-01,  2.0475e-01,  4.3782e-02, -1.2909e-01, -2.8440e-02,\n",
    "         -2.9444e-02, -1.9192e-01,  1.9533e-02, -6.8360e-03, -3.4114e-02,\n",
    "          2.5344e-02,  9.4265e-02,  1.0698e-01,  1.0703e-01,  6.3482e-03,\n",
    "          4.3164e-01,  1.0142e-01,  4.1497e-02,  2.1196e-01, -5.6719e-02,\n",
    "         -9.3606e-02, -8.5046e-02, -1.7235e-01, -2.9160e-01,  4.7828e-02,\n",
    "         -2.5945e-02,  2.4157e-01, -1.2213e-01, -2.0752e-02,  9.7038e-02,\n",
    "          3.4821e-02, -1.8796e-01, -2.0718e-01,  9.7396e-02,  1.1684e-01,\n",
    "         -9.9320e-02, -4.9574e-02,  1.5355e-01, -3.1482e-01,  1.9748e-01,\n",
    "         -8.9494e-02, -1.3103e-01,  2.0197e-03, -1.8131e-01, -3.5504e-03,\n",
    "          1.3379e-01,  5.5091e-02,  1.3466e-01,  1.2647e-01,  7.1849e-02,\n",
    "         -2.4761e-02,  3.7153e-01,  3.8314e-02, -6.3391e-02, -1.3208e-01,\n",
    "         -1.1597e-01, -8.6217e-02,  1.3040e-02,  3.4986e-01,  2.1997e-02,\n",
    "         -6.1772e-02,  3.9376e-02, -3.2735e-01,  2.1495e-01, -3.5137e-02,\n",
    "         -1.6507e-01, -1.1525e-02,  4.4876e-02,  1.1829e-02, -8.2609e-03,\n",
    "          2.7066e-01,  5.8643e-02, -4.9348e-02, -2.5994e-02,  3.0485e-01,\n",
    "          8.7909e-03, -4.9846e-02, -1.5638e-01, -2.8386e-01, -2.4185e-02,\n",
    "         -1.6744e-02, -1.3982e-03, -2.8604e-02,  2.1444e-01,  1.0324e-01,\n",
    "          5.0863e-02, -7.6575e-02,  2.4376e-01,  8.4093e-02, -1.6324e-01,\n",
    "          2.2216e-01],\n",
    "        [-4.0470e-02,  3.2170e-02,  7.3006e-02,  8.6206e-02,  4.1420e-02,\n",
    "         -3.8547e-02,  2.4198e-02, -8.4722e-02,  1.0197e-02,  8.7263e-02,\n",
    "         -1.2745e-01, -1.9789e-04,  2.6842e-02,  1.3672e-02,  8.5757e-02,\n",
    "         -6.1096e-02,  2.2066e-01,  2.5827e-01, -6.0008e-03, -1.2356e-01,\n",
    "          3.6031e-01,  3.2153e-02,  3.0802e-02, -1.8758e-01, -3.6237e-02,\n",
    "          3.1618e-02,  8.6423e-02,  2.3901e-02,  2.2253e-01,  1.2660e-01,\n",
    "         -2.3803e-02, -1.3003e-01, -3.4190e-01, -2.2100e-01,  4.4635e-02,\n",
    "          1.7485e-01, -5.4655e-02,  2.0100e-01, -1.2380e-01, -2.6847e-01,\n",
    "         -1.0482e-02,  1.7076e-01, -2.6683e-01, -5.2096e-02, -2.9916e-01,\n",
    "         -2.7215e-01, -1.9875e-02, -2.2110e-02, -9.6906e-02, -5.0813e-02,\n",
    "         -3.0514e-03, -1.4289e-01, -1.2275e-01, -2.1364e-01, -1.5910e-01,\n",
    "         -1.8805e-01, -9.3259e-03, -1.4689e-01, -1.8086e-01,  1.6982e-01,\n",
    "          1.9228e-01,  1.1483e-01, -1.0245e-01, -5.3320e-02,  3.1561e-02,\n",
    "          3.0372e-02,  1.0566e-01, -9.4280e-02,  4.8850e-02, -1.8293e-02,\n",
    "          7.4666e-02, -1.5537e-01,  1.1456e-01,  1.4605e-01,  5.2352e-03,\n",
    "         -9.2179e-02,  1.3659e-01, -2.5743e-02, -1.1507e-01, -2.2098e-01,\n",
    "         -1.0834e-01,  1.0111e-01,  7.5301e-02, -3.6238e-02,  4.9994e-02,\n",
    "          2.3608e-01,  3.3682e-01,  2.2014e-01, -1.3645e-01, -1.1892e-01,\n",
    "          5.4488e-02, -3.4124e-02, -1.6638e-01,  6.7973e-02,  3.8814e-01,\n",
    "          3.3219e-02],\n",
    "        [ 1.1308e-01,  8.2292e-02,  1.9947e-02,  7.4375e-02, -2.9953e-02,\n",
    "         -1.1148e-01, -1.7058e-01, -1.5541e-01, -3.2702e-02, -5.6337e-02,\n",
    "         -7.1867e-02, -3.5793e-02, -1.3938e-02, -2.5947e-02, -3.7676e-02,\n",
    "          1.5220e-02,  2.7737e-02, -3.2479e-01, -3.6800e-01, -1.6443e-01,\n",
    "         -1.9411e-01, -2.9360e-01, -2.3616e-01, -1.7911e-01,  9.9358e-02,\n",
    "          7.1145e-02,  4.3900e-02,  5.9845e-02,  2.1982e-01,  1.0244e-01,\n",
    "          1.1474e-01,  9.6551e-02,  2.4892e-01,  1.5009e-01,  3.1459e-01,\n",
    "          2.7056e-01,  1.2293e-01,  5.0492e-02,  2.0291e-01,  1.6220e-01,\n",
    "         -1.1057e-01, -3.9266e-02,  4.2088e-02, -1.4810e-01, -2.9828e-02,\n",
    "          2.0666e-01,  8.4155e-02,  2.3866e-02,  1.1577e-02,  1.3587e-01,\n",
    "         -1.5831e-02, -7.2568e-03,  8.8296e-03,  1.1342e-01, -1.7735e-01,\n",
    "         -2.4566e-03, -2.8070e-02,  8.8844e-02,  2.1838e-01,  2.2268e-01,\n",
    "          5.8904e-03,  8.9845e-02, -8.0377e-02, -2.2781e-01, -1.3939e-01,\n",
    "         -2.7587e-01, -2.7923e-01, -1.6532e-01, -8.0520e-02, -3.0224e-02,\n",
    "          3.9469e-03, -2.8205e-02,  1.9692e-02,  6.3267e-02,  8.2390e-02,\n",
    "         -4.6643e-02,  2.7274e-02, -1.2639e-01,  7.1896e-02,  4.0444e-02,\n",
    "         -5.0740e-02, -7.1817e-02,  2.0229e-01,  1.3291e-01,  1.9402e-01,\n",
    "         -3.9710e-02,  6.7428e-02, -3.2232e-02, -1.3801e-01, -1.6778e-01,\n",
    "          1.5196e-01, -5.1015e-02, -1.1681e-01, -9.7400e-02, -3.2690e-01,\n",
    "         -1.8869e-02],\n",
    "        [ 7.0306e-02,  6.1576e-02, -1.8232e-01, -1.7878e-01,  1.3253e-02,\n",
    "          2.3962e-02, -1.3051e-01, -4.0697e-02, -6.2435e-02, -7.1635e-03,\n",
    "          2.7667e-02,  1.9461e-01,  2.0451e-02,  8.9417e-02,  1.1847e-01,\n",
    "         -5.3932e-02, -1.2022e-01,  1.1547e-02,  1.9381e-01,  1.5139e-01,\n",
    "         -5.4729e-02,  7.7917e-02, -7.4178e-02,  2.0149e-01,  1.4637e-01,\n",
    "          8.5925e-02,  1.2594e-01,  6.4392e-02,  3.2194e-01, -1.5965e-02,\n",
    "          8.4646e-02, -1.0490e-01,  7.9911e-02,  6.7037e-03, -2.9646e-01,\n",
    "         -4.2846e-01,  2.2150e-01,  1.5781e-02, -3.7039e-01, -4.2994e-01,\n",
    "         -3.0347e-02, -1.1471e-01,  3.8932e-02,  8.9337e-02, -9.4180e-02,\n",
    "         -1.2295e-01, -6.8640e-02,  1.6956e-01, -1.3404e-02,  2.3860e-02,\n",
    "         -2.0042e-02, -1.9561e-01, -9.8455e-02, -2.1969e-02,  1.0084e-01,\n",
    "          9.7036e-02,  1.4615e-01,  4.7965e-02,  6.6582e-02,  8.1162e-02,\n",
    "          1.6618e-01, -8.3714e-02, -4.5869e-02,  6.1326e-02, -9.2739e-02,\n",
    "          8.3452e-02,  2.3759e-01,  4.0640e-01, -3.6018e-02, -6.3108e-02,\n",
    "          1.3316e-02,  1.6075e-01,  9.0032e-02,  1.1810e-03, -6.5371e-02,\n",
    "         -1.0350e-01,  3.0301e-01,  5.2208e-02, -1.5181e-01, -1.1825e-01,\n",
    "         -1.2844e-01, -9.7420e-02, -2.0221e-02,  1.9430e-01, -1.3931e-01,\n",
    "         -1.4453e-02, -2.1196e-01, -2.1577e-01, -2.5744e-02, -1.5737e-01,\n",
    "         -9.2890e-02, -2.2314e-01, -4.0968e-02,  1.8771e-01,  2.2581e-01,\n",
    "          5.7778e-02],\n",
    "        [-4.7018e-02,  1.4302e-01, -4.4327e-02, -1.3050e-01, -6.3246e-02,\n",
    "         -3.3748e-02, -2.9534e-02, -2.7970e-02,  1.0837e-01,  1.5011e-02,\n",
    "         -4.6480e-02,  4.0015e-02, -5.1142e-02,  6.3716e-03,  8.8488e-03,\n",
    "         -1.2526e-02, -2.4487e-01, -3.3652e-01, -2.1464e-01,  3.4777e-01,\n",
    "         -2.4318e-01, -2.8408e-01,  7.7136e-03,  2.7977e-01, -3.0212e-01,\n",
    "         -1.0295e-01, -4.7130e-02, -1.9075e-01,  4.6135e-02, -1.4775e-01,\n",
    "          9.5834e-02, -1.9962e-03,  1.2141e-01,  3.9361e-02,  7.2808e-02,\n",
    "         -2.5940e-01,  9.2774e-03,  1.8890e-02, -2.4703e-01, -1.3669e-01,\n",
    "          1.0613e-01,  6.1018e-02, -1.3803e-01,  8.9025e-02,  3.1226e-01,\n",
    "          1.0175e-01, -9.8680e-02, -6.1244e-02, -5.2437e-02, -1.4456e-01,\n",
    "         -1.3720e-01, -1.6016e-01, -4.5704e-02,  4.8326e-02,  1.4195e-01,\n",
    "          2.6608e-01, -9.2636e-02,  1.4359e-01,  2.8097e-02, -4.2358e-02,\n",
    "         -2.0102e-01, -1.3536e-01,  1.0053e-01,  1.7321e-02, -6.5864e-02,\n",
    "         -1.5057e-01,  7.4170e-02,  6.1726e-02, -5.4822e-02, -4.1805e-02,\n",
    "          2.0091e-01,  3.2085e-01, -1.5857e-01, -3.0637e-02,  2.7448e-02,\n",
    "         -3.8163e-02, -1.8654e-01,  7.7815e-02,  1.9332e-02, -4.6774e-02,\n",
    "          5.4963e-03, -7.2700e-03, -4.6082e-02, -1.3000e-02,  1.7182e-01,\n",
    "         -2.3271e-01, -2.5716e-01, -1.0892e-01,  6.2360e-02,  2.8512e-01,\n",
    "         -1.5542e-01,  3.2691e-01,  3.1535e-02,  3.9158e-02,  3.5225e-01,\n",
    "          3.0335e-01],\n",
    "        [ 2.0869e-02,  1.3971e-01,  1.2722e-01,  5.9499e-02,  7.9136e-02,\n",
    "          3.2876e-02,  6.3345e-02, -1.0381e-01,  7.0327e-02, -5.6021e-02,\n",
    "          4.7694e-02, -1.6968e-01,  3.7234e-02,  5.0338e-02, -1.8386e-02,\n",
    "         -1.2451e-01,  2.1683e-01,  1.2961e-01,  5.9455e-02, -5.5498e-02,\n",
    "          2.8876e-01,  1.4261e-01,  1.8487e-01, -9.5894e-02,  2.4031e-01,\n",
    "         -8.3247e-02, -1.6296e-02,  5.4169e-02, -2.9342e-02, -2.6003e-01,\n",
    "          2.8941e-02,  1.6257e-01, -5.3330e-02, -4.9601e-02, -1.4858e-03,\n",
    "         -1.3697e-01, -1.5934e-01, -7.5391e-02,  3.2852e-01,  1.2177e-01,\n",
    "          8.7232e-02,  9.4942e-02,  2.7874e-01,  1.3071e-01, -1.8636e-01,\n",
    "          2.8442e-01,  1.0103e-01, -1.9614e-01,  7.8748e-02,  1.8730e-01,\n",
    "          1.2427e-01,  1.6539e-01, -1.5243e-02,  2.1468e-01,  2.8279e-02,\n",
    "         -6.2101e-02, -1.3047e-01, -1.0014e-01,  1.8369e-01,  1.3831e-01,\n",
    "         -5.8661e-02,  1.1693e-01, -9.1601e-02, -3.0088e-01, -9.1717e-03,\n",
    "          1.1009e-01,  7.9640e-02,  1.3871e-01,  7.3086e-04, -1.0700e-01,\n",
    "          1.7048e-01,  5.9855e-02,  2.8726e-01,  1.2809e-01, -1.5109e-01,\n",
    "          7.2226e-02, -8.8942e-02,  1.4850e-02,  1.6172e-02, -7.1029e-03,\n",
    "         -2.4776e-01, -2.4779e-01, -1.2215e-01, -4.2312e-02, -1.3449e-01,\n",
    "          6.6630e-02, -1.1936e-01,  3.1432e-02,  1.8863e-01, -1.8296e-01,\n",
    "         -1.1521e-01, -2.9818e-03, -8.0898e-02, -9.3964e-02, -4.4481e-01,\n",
    "         -3.7265e-01],\n",
    "        [-1.0915e-01,  6.0487e-03, -2.6825e-01,  6.0598e-02, -2.4549e-02,\n",
    "          8.8001e-02, -2.4926e-02, -1.2354e-01,  3.0695e-02,  7.9460e-02,\n",
    "         -9.7738e-02, -3.3442e-02, -5.9938e-02,  7.3952e-02, -2.8283e-02,\n",
    "          2.1566e-01, -8.4645e-02,  1.7345e-01,  2.8290e-02,  7.3085e-02,\n",
    "         -7.0060e-02, -5.4586e-02, -7.1739e-02, -9.9966e-02,  1.2217e-01,\n",
    "          2.4147e-01, -8.9706e-02,  2.0175e-01, -3.4084e-01,  1.6492e-01,\n",
    "         -2.5207e-02, -1.4302e-01,  1.4863e-01,  1.0918e-01, -1.1954e-01,\n",
    "         -2.8527e-02, -5.9190e-03, -1.1985e-01, -2.6616e-01,  3.9518e-02,\n",
    "         -4.7514e-02,  3.1681e-03, -1.4639e-01, -1.1669e-01,  2.8215e-01,\n",
    "         -8.4671e-02, -1.5599e-01, -1.0251e-02,  4.0808e-02, -1.6813e-01,\n",
    "         -1.3070e-01,  1.0655e-02,  5.2427e-02, -2.2039e-01,  8.3956e-02,\n",
    "         -1.7240e-01,  9.6387e-02, -3.7873e-02,  5.4630e-02, -2.1626e-02,\n",
    "         -1.2024e-01, -3.7603e-01, -6.9198e-02,  1.9696e-02,  7.6373e-02,\n",
    "          8.2508e-02,  1.3725e-01, -2.6848e-02, -2.6020e-01, -6.3209e-02,\n",
    "         -5.6235e-02, -1.9267e-02, -1.3467e-01, -7.5441e-02,  2.9640e-02,\n",
    "         -1.5758e-02, -3.4012e-02, -2.0289e-01, -1.5440e-01, -1.5100e-01,\n",
    "          2.7721e-01,  5.4123e-03,  1.4862e-01,  1.6594e-01, -4.2599e-02,\n",
    "         -6.9132e-02,  2.1144e-01,  3.0043e-01,  1.2972e-01,  3.3903e-01,\n",
    "         -1.0348e-01,  6.1745e-03,  6.2743e-02, -1.0543e-01,  2.4746e-01,\n",
    "          5.3212e-02],\n",
    "        [-1.5756e-01, -1.5974e-01, -1.0484e-01,  1.4071e-01, -1.0550e-01,\n",
    "         -3.1597e-02,  8.8742e-02,  1.6610e-01, -4.4592e-02,  2.2722e-02,\n",
    "         -5.5368e-02,  8.3352e-02,  6.4819e-02, -3.4194e-02, -1.5154e-02,\n",
    "         -6.0946e-02, -5.5572e-02,  2.5791e-02,  2.1878e-01,  1.1456e-02,\n",
    "         -3.1748e-01, -1.5705e-01,  8.7747e-04,  9.1801e-02,  8.3374e-02,\n",
    "          1.9374e-01,  1.0021e-01, -2.8527e-02,  3.2867e-01,  1.5340e-01,\n",
    "         -1.1741e-01, -2.0426e-01, -8.9856e-04, -1.6089e-01, -2.9962e-01,\n",
    "         -1.1050e-01,  6.5009e-02, -1.3030e-02, -4.9626e-02,  1.8599e-01,\n",
    "         -3.2962e-03, -6.2939e-02,  1.3340e-01,  1.0585e-01, -1.4301e-01,\n",
    "          5.3546e-02,  1.1987e-01,  2.4238e-01,  3.1716e-02, -1.2747e-01,\n",
    "          1.5714e-02,  1.1639e-01, -1.3231e-01, -3.2741e-01, -4.6124e-02,\n",
    "          1.1899e-01, -2.0949e-01,  1.3598e-01, -2.0763e-01, -7.2700e-02,\n",
    "         -3.2497e-03,  1.6921e-01, -1.2114e-01, -3.4188e-01,  1.2861e-01,\n",
    "          1.5374e-01,  3.4947e-02,  6.9252e-03,  4.5859e-02,  1.0792e-03,\n",
    "         -2.4421e-01, -3.7357e-01, -2.2682e-02,  9.4811e-03,  6.7646e-02,\n",
    "         -1.7351e-01, -7.8633e-02,  9.0672e-02,  1.6199e-01,  3.8564e-03,\n",
    "          8.4979e-02,  2.3677e-01,  5.5735e-02, -8.2782e-02, -1.2861e-01,\n",
    "         -1.8049e-01,  3.5351e-01,  8.1959e-02, -4.8443e-01,  1.7036e-01,\n",
    "          1.7495e-01,  2.1059e-02, -1.4918e-01, -5.4696e-03, -2.8341e-01,\n",
    "         -3.7555e-01]]\n",
    "fc_bias = [-0.1474,  0.0831, -0.0449, -0.1405,  0.0600,  0.0787, -0.0340, -0.0387,\n",
    "        -0.1353,  0.0976]\n",
    " #signed int => unsigned int\n",
    "from fxpmath import Fxp\n",
    "for k in range(10):\n",
    "        b = Fxp(fc_bias[k], signed=True, n_word=16, n_frac=8)\n",
    "        print(b.hex())\n",
    "        #print(b.val)\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(96):\n",
    "        \n",
    "        x = Fxp(fc_weight[i][j], signed=True, n_word=8, n_frac=4)\n",
    "        #print(x)         # float value\n",
    "        #print(x.bin())   # binary representation\n",
    "        print(x.hex())\n",
    "        #print(x.val)\n",
    "        \n",
    "        \n",
    "\n",
    "#print(int_fc_weight)\n",
    "\n",
    "#np.savetxt('fc_weight.mem', int_fc_weight, fmt='%1.2x',delimiter = \" \")\n",
    "#np.savetxt('fc_bias.mem', float_fc_bias, fmt='%1.2x',delimiter = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 3, 24, 24)\n",
      "(16, 3, 12, 12)\n",
      "(16, 3, 8, 8)\n",
      "(16, 3, 4, 4)\n",
      "(16, 48)\n",
      "(16, 10)\n"
     ]
    }
   ],
   "source": [
    "######################### 신경망 통과 값 추출 ###########################\n",
    "print(np.shape(model.conv1_out_np))\n",
    "np.savetxt('out_conv1_value_1.txt', model.conv1_out_np[0][0]*128, fmt='%1.5d',delimiter = \" \")\n",
    "np.savetxt('out_conv1_value_2.txt', model.conv1_out_np[0][1]*128, fmt='%1.5d',delimiter = \" \")\n",
    "np.savetxt('out_conv1_value_3.txt', model.conv1_out_np[0][2]*128, fmt='%1.5d',delimiter = \" \")\n",
    "\n",
    "print(np.shape(model.mp1_out_np))\n",
    "np.savetxt('out_mp1_value_1.txt', model.mp1_out_np[0][0]*128, fmt='%1.5d',delimiter = \" \")\n",
    "np.savetxt('out_mp1_value_2.txt', model.mp1_out_np[0][1]*128, fmt='%1.5d',delimiter = \" \")\n",
    "np.savetxt('out_mp1_value_3.txt', model.mp1_out_np[0][2]*128, fmt='%1.5d',delimiter = \" \")\n",
    "\n",
    "print(np.shape(model.conv3_out_np))\n",
    "np.savetxt('out_conv2_value_1.txt', model.conv3_out_np[0][0]*128, fmt='%1.5d',delimiter = \" \")\n",
    "np.savetxt('out_conv2_value_2.txt', model.conv3_out_np[0][1]*128, fmt='%1.5d',delimiter = \" \")\n",
    "np.savetxt('out_conv2_value_3.txt', model.conv3_out_np[0][2]*128, fmt='%1.5d',delimiter = \" \")\n",
    "\n",
    "print(np.shape(model.mp2_out_np))\n",
    "np.savetxt('out_mp2_value_1.txt', model.mp2_out_np[0][0]*128, fmt='%1.5d',delimiter = \" \")\n",
    "np.savetxt('out_mp2_value_2.txt', model.mp2_out_np[0][1]*128, fmt='%1.5d',delimiter = \" \")\n",
    "np.savetxt('out_mp2_value_3.txt', model.mp2_out_np[0][2]*128, fmt='%1.5d',delimiter = \" \")\n",
    "\n",
    "print(np.shape(model.fc_in_np))\n",
    "np.savetxt('fc_in_value.txt', model.fc_in_np*128, fmt='%1.5d',delimiter = \" \")\n",
    "\n",
    "print(np.shape(model.fc_out_np))\n",
    "np.savetxt('fc_out_value.txt', model.fc_out_np*128, fmt='%1.5d',delimiter = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Input\n",
      "[[[0.       0.       0.       0.       0.       0.217029 0.907689\n",
      "   0.630675 0.       0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       0.       1.444534 2.935081\n",
      "   1.454854 0.       0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       0.       2.799465 4.030156\n",
      "   2.134425 0.       0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       0.034406 3.356139 4.508522\n",
      "   1.926755 0.       0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       0.226475 3.496622 4.584289\n",
      "   1.454748 0.       0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       0.530351 3.7459   4.344213\n",
      "   0.895292 0.       0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       1.16573  4.174893 4.093958\n",
      "   0.303794 0.       0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       1.776142 4.344497 3.684725\n",
      "   0.       0.       0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       2.437443 4.511919 3.281525\n",
      "   0.       0.       0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       2.861234 4.199187 2.346806\n",
      "   0.       0.       0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       2.900786 4.195368 1.420527\n",
      "   0.       0.       0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       1.522478 2.375021 0.82321\n",
      "   0.       0.       0.       0.       0.      ]]\n",
      "\n",
      " [[0.       0.       0.       0.       0.       0.       0.\n",
      "   0.065031 0.311637 0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       0.       0.       0.\n",
      "   1.23693  1.668517 0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       0.       0.       0.\n",
      "   2.583566 2.544075 0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       0.       0.       0.\n",
      "   2.730523 2.419295 0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       0.       0.       0.\n",
      "   2.738858 2.044446 0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       0.       0.       0.83836\n",
      "   2.797417 1.45222  0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       0.       0.       1.567858\n",
      "   2.677736 0.7958   0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       0.       0.       2.039324\n",
      "   2.736632 0.493803 0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       0.       0.       2.375309\n",
      "   2.798339 0.161888 0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       0.       0.       2.818319\n",
      "   2.628942 0.       0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       0.       0.019138 2.825513\n",
      "   1.946506 0.       0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       0.       0.426708 1.562003\n",
      "   0.541563 0.       0.       0.       0.      ]]\n",
      "\n",
      " [[0.       0.       0.       0.       0.       0.       0.\n",
      "   0.       0.       0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       0.       0.       0.\n",
      "   0.       0.       0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       0.018926 0.504196 0.717967\n",
      "   0.39205  0.142507 0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       0.097085 1.228348 1.593388\n",
      "   0.778542 0.348923 0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       0.106915 1.267343 1.653161\n",
      "   0.776736 0.214996 0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       0.22121  1.009319 1.385548\n",
      "   0.941645 0.505882 0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       0.357101 1.051962 1.203354\n",
      "   0.72838  0.297186 0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       0.539851 0.963361 0.828111\n",
      "   0.469033 0.155245 0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       0.767624 1.439448 0.983448\n",
      "   0.551358 0.264263 0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       0.966309 1.482581 1.027152\n",
      "   0.744947 0.133143 0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       1.867275 2.626462 1.932018\n",
      "   0.640798 0.054892 0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       1.782573 3.094548 2.807754\n",
      "   0.81758  0.       0.       0.       0.      ]]]\n",
      "Original Weight1\n",
      "[[[[ 0.066241  0.313241  0.345499  0.219977  0.160031]\n",
      "   [-0.26574   0.075896  0.102618  0.358113  0.272528]\n",
      "   [-0.198488 -0.082013  0.370592  0.571731  0.331933]\n",
      "   [-0.154596 -0.111936  0.200181  0.25042   0.359928]\n",
      "   [-0.293524  0.140791  0.262767  0.455675  0.428726]]]\n",
      "\n",
      "\n",
      " [[[-0.007099  0.368824  0.109683 -0.087628  0.055519]\n",
      "   [ 0.233972  0.377606  0.17698  -0.228463 -0.353022]\n",
      "   [ 0.245173  0.466966 -0.082473 -0.240101 -0.46035 ]\n",
      "   [ 0.456661  0.28356  -0.256038 -0.405105 -0.165466]\n",
      "   [ 0.421176  0.017316 -0.189003 -0.227552 -0.170885]]]\n",
      "\n",
      "\n",
      " [[[ 0.520159  0.622913  0.623146  0.342179  0.417766]\n",
      "   [ 0.123245  0.487419  0.371862  0.32967   0.281675]\n",
      "   [-0.097955 -0.063705  0.129275  0.206857  0.358887]\n",
      "   [-0.362018 -0.41056  -0.408132 -0.114659  0.119146]\n",
      "   [-0.429784 -0.393344 -0.308201 -0.488406 -0.217808]]]]\n",
      "Original bias1\n",
      "[-0.138461 -0.121583 -0.002223]\n",
      "Original Weight2\n",
      "[[[[-0.089214 -0.199801 -0.08481  -0.191061 -0.324694]\n",
      "   [ 0.001711  0.045712 -0.37653  -0.389366 -0.124824]\n",
      "   [-0.096015 -0.070604 -0.204915 -0.189684  0.031029]\n",
      "   [ 0.092029  0.014611 -0.170218 -0.060216 -0.062117]\n",
      "   [ 0.066156  0.334924  0.041925 -0.125559  0.125611]]]\n",
      "\n",
      "\n",
      " [[[ 0.182019  0.379085  0.42186   0.428359  0.104908]\n",
      "   [-0.111641  0.231715  0.300777  0.231023  0.119187]\n",
      "   [-0.161877 -0.117551 -0.12118  -0.261154  0.227682]\n",
      "   [ 0.059223  0.228163  0.070562  0.008904  0.153886]\n",
      "   [-0.063773 -0.154627 -0.141288 -0.077258  0.237332]]]\n",
      "\n",
      "\n",
      " [[[-0.270094 -0.289893 -0.363589 -0.061274 -0.365177]\n",
      "   [ 0.012261  0.049535 -0.341483 -0.547569 -0.110879]\n",
      "   [ 0.059823  0.003731  0.071253 -0.011243  0.091054]\n",
      "   [-0.10618  -0.111803 -0.112048 -0.210902 -0.184458]\n",
      "   [ 0.161545  0.304949  0.11461   0.123948 -0.140302]]]\n",
      "\n",
      "\n",
      " [[[-0.129354 -0.047212  0.017971 -0.031324 -0.04779 ]\n",
      "   [-0.228369  0.040575 -0.202766 -0.503989 -0.428539]\n",
      "   [ 0.12192   0.263593 -0.222531 -0.237065 -0.164073]\n",
      "   [ 0.286386 -0.157553 -0.305122 -0.126991 -0.217318]\n",
      "   [ 0.027727  0.204895  0.011249  0.031022  0.105796]]]\n",
      "\n",
      "\n",
      " [[[ 0.019326 -0.023003  0.074736 -0.117006 -0.028332]\n",
      "   [-0.128834  0.048729 -0.030288  0.199291  0.280507]\n",
      "   [ 0.215742  0.292069  0.490962  0.462055  0.165567]\n",
      "   [ 0.144992  0.103116  0.254426  0.105052 -0.080085]\n",
      "   [ 0.278167  0.187565  0.129122 -0.214287 -0.133013]]]\n",
      "\n",
      "\n",
      " [[[-0.052061  0.087167  0.088899 -0.216658  0.110404]\n",
      "   [-0.071747 -0.004857 -0.023868 -0.076073  0.059055]\n",
      "   [-0.034686 -0.046032 -0.21936  -0.204594 -0.30831 ]\n",
      "   [ 0.268733 -0.068741  0.039265 -0.072024 -0.243838]\n",
      "   [-0.075427  0.171432  0.253298  0.15423   0.021953]]]\n",
      "\n",
      "\n",
      " [[[-0.044826  0.097164 -0.04121  -0.100345 -0.283716]\n",
      "   [ 0.106526  0.063265  0.032226 -0.311034 -0.111149]\n",
      "   [-0.076086  0.102961 -0.210449  0.06118  -0.059366]\n",
      "   [ 0.065251  0.271353  0.275885  0.07304   0.007134]\n",
      "   [ 0.129665  0.083689 -0.029753  0.22105   0.398072]]]\n",
      "\n",
      "\n",
      " [[[-0.191055  0.10229   0.221552  0.294799  0.319599]\n",
      "   [ 0.118688  0.040152  0.217606  0.497831  0.389047]\n",
      "   [ 0.210191  0.238094  0.297576  0.414765  0.309728]\n",
      "   [ 0.090009  0.481819  0.659795  0.538533 -0.022743]\n",
      "   [-0.225799  0.195256  0.319353  0.123485  0.032475]]]\n",
      "\n",
      "\n",
      " [[[-0.112302  0.010373 -0.217704 -0.012051  0.347124]\n",
      "   [-0.041569  0.045329  0.156967  0.092549 -0.00223 ]\n",
      "   [-0.25742   0.145174  0.190421 -0.041933  0.193496]\n",
      "   [ 0.119089 -0.258153 -0.189172 -0.277693  0.143076]\n",
      "   [-0.187574 -0.15411  -0.268989 -0.23317  -0.259952]]]]\n",
      "Original Weight3\n",
      "[[[[ 0.364215]]\n",
      "\n",
      "  [[-0.005797]]\n",
      "\n",
      "  [[ 0.059912]]\n",
      "\n",
      "  [[-0.001403]]\n",
      "\n",
      "  [[ 0.129101]]\n",
      "\n",
      "  [[-0.053502]]\n",
      "\n",
      "  [[-0.176146]]\n",
      "\n",
      "  [[ 0.018775]]\n",
      "\n",
      "  [[-0.139774]]]\n",
      "\n",
      "\n",
      " [[[ 0.254235]]\n",
      "\n",
      "  [[-0.7631  ]]\n",
      "\n",
      "  [[ 0.520889]]\n",
      "\n",
      "  [[-0.064566]]\n",
      "\n",
      "  [[-0.612489]]\n",
      "\n",
      "  [[-0.108406]]\n",
      "\n",
      "  [[ 0.364301]]\n",
      "\n",
      "  [[ 1.275608]]\n",
      "\n",
      "  [[-0.573709]]]\n",
      "\n",
      "\n",
      " [[[-0.590038]]\n",
      "\n",
      "  [[ 0.364954]]\n",
      "\n",
      "  [[-0.624366]]\n",
      "\n",
      "  [[-0.841229]]\n",
      "\n",
      "  [[ 0.579939]]\n",
      "\n",
      "  [[-0.520295]]\n",
      "\n",
      "  [[ 0.56087 ]]\n",
      "\n",
      "  [[-0.646341]]\n",
      "\n",
      "  [[-0.603463]]]]\n",
      "Original bias3\n",
      "[-0.169512 -0.263137 -0.550158]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\OneDrive - 광운대학교\\Capstone\\CNN\\my\\lenet5\\MNIST_CNN.ipynb 셀 8\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20%EA%B4%91%EC%9A%B4%EB%8C%80%ED%95%99%EA%B5%90/Capstone/CNN/my/lenet5/MNIST_CNN.ipynb#X10sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m8\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20%EA%B4%91%EC%9A%B4%EB%8C%80%ED%95%99%EA%B5%90/Capstone/CNN/my/lenet5/MNIST_CNN.ipynb#X10sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m8\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20%EA%B4%91%EC%9A%B4%EB%8C%80%ED%95%99%EA%B5%90/Capstone/CNN/my/lenet5/MNIST_CNN.ipynb#X10sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m         orig_output_calc_1[c][i][j] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (orig_input_1[i:i\u001b[39m+\u001b[39m\u001b[39m5\u001b[39m, j:j\u001b[39m+\u001b[39m\u001b[39m5\u001b[39m] \u001b[39m*\u001b[39m orig_weight[c][\u001b[39m0\u001b[39;49m])\u001b[39m.\u001b[39msum()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20%EA%B4%91%EC%9A%B4%EB%8C%80%ED%95%99%EA%B5%90/Capstone/CNN/my/lenet5/MNIST_CNN.ipynb#X10sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m         orig_output_calc_2[c][i][j] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (orig_input_2[i:i\u001b[39m+\u001b[39m\u001b[39m5\u001b[39m, j:j\u001b[39m+\u001b[39m\u001b[39m5\u001b[39m] \u001b[39m*\u001b[39m orig_weight[c][\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39msum()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20%EA%B4%91%EC%9A%B4%EB%8C%80%ED%95%99%EA%B5%90/Capstone/CNN/my/lenet5/MNIST_CNN.ipynb#X10sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m         orig_output_calc_3[c][i][j] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (orig_input_3[i:i\u001b[39m+\u001b[39m\u001b[39m5\u001b[39m, j:j\u001b[39m+\u001b[39m\u001b[39m5\u001b[39m] \u001b[39m*\u001b[39m orig_weight[c][\u001b[39m2\u001b[39m])\u001b[39m.\u001b[39msum()         \n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "# 원래 입력 / 가중치 / 바이어스 출력\n",
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions(precision=6)\n",
    "\n",
    "print(\"Original Input\")\n",
    "print(model.mp1_out_np[0])\n",
    "orig_input_1 = model.mp1_out_np[0][0]\n",
    "orig_input_2 = model.mp1_out_np[0][1]\n",
    "orig_input_3 = model.mp1_out_np[0][2]\n",
    "\n",
    "print(\"Original Weight1\")\n",
    "print(model.conv1.weight.detach().numpy())\n",
    "orig_weight = model.conv1.weight.detach().numpy()\n",
    "\n",
    "print(\"Original bias1\")\n",
    "print(model.conv1.bias.detach().numpy())\n",
    "orig_weight = model.conv1.bias.detach().numpy()\n",
    "\n",
    "print(\"Original Weight2\")\n",
    "print(model.conv2.weight.detach().numpy())\n",
    "orig_weight = model.conv2.weight.detach().numpy()\n",
    "\n",
    "print(\"Original Weight3\")\n",
    "print(model.conv3.weight.detach().numpy())\n",
    "orig_bias = model.conv3.weight.detach().numpy()\n",
    "\n",
    "print(\"Original bias3\")\n",
    "print(model.conv3.bias.detach().numpy())\n",
    "orig_weight = model.conv3.bias.detach().numpy()\n",
    "\n",
    "orig_output_calc_1 = np.zeros((3,8,8))\n",
    "orig_output_calc_2 = np.zeros((3,8,8))\n",
    "orig_output_calc_3 = np.zeros((3,8,8))\n",
    "orig_output_calc = np.zeros((3,8,8))\n",
    "\n",
    "for c in range(3):\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            orig_output_calc_1[c][i][j] += (orig_input_1[i:i+5, j:j+5] * orig_weight[c][0]).sum()\n",
    "            orig_output_calc_2[c][i][j] += (orig_input_2[i:i+5, j:j+5] * orig_weight[c][1]).sum()\n",
    "            orig_output_calc_3[c][i][j] += (orig_input_3[i:i+5, j:j+5] * orig_weight[c][2]).sum()         \n",
    "            orig_output_calc[c][i][j] = orig_output_calc_1[c][i][j] + orig_output_calc_2[c][i][j] + orig_output_calc_3[c][i][j] + orig_bias[c]\n",
    "        \n",
    "print(\"\\nBias : \")\n",
    "print(orig_bias)\n",
    "print(\"\\nCalc Value :\\n\")\n",
    "print(orig_output_calc) \n",
    "print(\"\\nReal Value :\\n\")\n",
    "print(model.conv2_out_np[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution Input\n",
      "(16, 3, 12, 12)\n",
      "[[[  0.   0.   0.   0.   0.  28. 116.  81.   0.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0.   0. 185. 376. 186.   0.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0.   0. 358. 516. 273.   0.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0.   4. 430. 577. 247.   0.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0.  29. 448. 587. 186.   0.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0.  68. 479. 556. 115.   0.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0. 149. 534. 524.  39.   0.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0. 227. 556. 472.   0.   0.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0. 312. 578. 420.   0.   0.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0. 366. 537. 300.   0.   0.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0. 371. 537. 182.   0.   0.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0. 195. 304. 105.   0.   0.   0.   0.   0.]]\n",
      "\n",
      " [[  0.   0.   0.   0.   0.   0.   0.   8.  40.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0.   0.   0.   0. 158. 214.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0.   0.   0.   0. 331. 326.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0.   0.   0.   0. 350. 310.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0.   0.   0.   0. 351. 262.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0.   0.   0. 107. 358. 186.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0.   0.   0. 201. 343. 102.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0.   0.   0. 261. 350.  63.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0.   0.   0. 304. 358.  21.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0.   0.   0. 361. 337.   0.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0.   0.   2. 362. 249.   0.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0.   0.  55. 200.  69.   0.   0.   0.   0.]]\n",
      "\n",
      " [[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0.   2.  65.  92.  50.  18.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0.  12. 157. 204. 100.  45.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0.  14. 162. 212.  99.  28.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0.  28. 129. 177. 121.  65.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0.  46. 135. 154.  93.  38.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0.  69. 123. 106.  60.  20.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0.  98. 184. 126.  71.  34.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0. 124. 190. 131.  95.  17.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0. 239. 336. 247.  82.   7.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0. 228. 396. 359. 105.   0.   0.   0.   0.]]]\n",
      "\n",
      "\n",
      "Convolution Weight\n",
      "(9, 1, 5, 5)\n",
      "[[[[-11. -26. -11. -24. -42.]\n",
      "   [  0.   6. -48. -50. -16.]\n",
      "   [-12.  -9. -26. -24.   4.]\n",
      "   [ 12.   2. -22.  -8.  -8.]\n",
      "   [  8.  43.   5. -16.  16.]]]\n",
      "\n",
      "\n",
      " [[[ 23.  49.  54.  55.  13.]\n",
      "   [-14.  30.  38.  30.  15.]\n",
      "   [-21. -15. -16. -33.  29.]\n",
      "   [  8.  29.   9.   1.  20.]\n",
      "   [ -8. -20. -18. -10.  30.]]]\n",
      "\n",
      "\n",
      " [[[-35. -37. -47.  -8. -47.]\n",
      "   [  2.   6. -44. -70. -14.]\n",
      "   [  8.   0.   9.  -1.  12.]\n",
      "   [-14. -14. -14. -27. -24.]\n",
      "   [ 21.  39.  15.  16. -18.]]]\n",
      "\n",
      "\n",
      " [[[-17.  -6.   2.  -4.  -6.]\n",
      "   [-29.   5. -26. -65. -55.]\n",
      "   [ 16.  34. -28. -30. -21.]\n",
      "   [ 37. -20. -39. -16. -28.]\n",
      "   [  4.  26.   1.   4.  14.]]]\n",
      "\n",
      "\n",
      " [[[  2.  -3.  10. -15.  -4.]\n",
      "   [-16.   6.  -4.  26.  36.]\n",
      "   [ 28.  37.  63.  59.  21.]\n",
      "   [ 19.  13.  33.  13. -10.]\n",
      "   [ 36.  24.  17. -27. -17.]]]\n",
      "\n",
      "\n",
      " [[[ -7.  11.  11. -28.  14.]\n",
      "   [ -9.  -1.  -3. -10.   8.]\n",
      "   [ -4.  -6. -28. -26. -39.]\n",
      "   [ 34.  -9.   5.  -9. -31.]\n",
      "   [-10.  22.  32.  20.   3.]]]\n",
      "\n",
      "\n",
      " [[[ -6.  12.  -5. -13. -36.]\n",
      "   [ 14.   8.   4. -40. -14.]\n",
      "   [-10.  13. -27.   8.  -8.]\n",
      "   [  8.  35.  35.   9.   1.]\n",
      "   [ 17.  11.  -4.  28.  51.]]]\n",
      "\n",
      "\n",
      " [[[-24.  13.  28.  38.  41.]\n",
      "   [ 15.   5.  28.  64.  50.]\n",
      "   [ 27.  30.  38.  53.  40.]\n",
      "   [ 12.  62.  84.  69.  -3.]\n",
      "   [-29.  25.  41.  16.   4.]]]\n",
      "\n",
      "\n",
      " [[[-14.   1. -28.  -2.  44.]\n",
      "   [ -5.   6.  20.  12.  -0.]\n",
      "   [-33.  19.  24.  -5.  25.]\n",
      "   [ 15. -33. -24. -36.  18.]\n",
      "   [-24. -20. -34. -30. -33.]]]]\n",
      "Convolution Bias\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\OneDrive - 광운대학교\\Capstone\\CNN\\my\\lenet5\\MNIST_CNN.ipynb 셀 9\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20%EA%B4%91%EC%9A%B4%EB%8C%80%ED%95%99%EA%B5%90/Capstone/CNN/my/lenet5/MNIST_CNN.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(weight)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20%EA%B4%91%EC%9A%B4%EB%8C%80%ED%95%99%EA%B5%90/Capstone/CNN/my/lenet5/MNIST_CNN.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mConvolution Bias\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20%EA%B4%91%EC%9A%B4%EB%8C%80%ED%95%99%EA%B5%90/Capstone/CNN/my/lenet5/MNIST_CNN.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39mshape(model\u001b[39m.\u001b[39;49mconv2\u001b[39m.\u001b[39;49mbias\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39mnumpy()))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20%EA%B4%91%EC%9A%B4%EB%8C%80%ED%95%99%EA%B5%90/Capstone/CNN/my/lenet5/MNIST_CNN.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m bias \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconv2\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy() \u001b[39m*\u001b[39m \u001b[39m128\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20%EA%B4%91%EC%9A%B4%EB%8C%80%ED%95%99%EA%B5%90/Capstone/CNN/my/lenet5/MNIST_CNN.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(bias)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=0)\n",
    "\n",
    "print(\"Convolution Input\")\n",
    "print(np.shape(model.mp1_out_np))\n",
    "_input = model.mp1_out_np[0] * 128\n",
    "print(_input)\n",
    "print('\\n')\n",
    "\n",
    "print(\"Convolution Weight\")\n",
    "print(np.shape(model.conv2.weight.detach().numpy()))\n",
    "weight = model.conv2.weight.detach().numpy() * 128\n",
    "print(weight)\n",
    "\n",
    "print(\"Convolution Bias\")\n",
    "print(np.shape(model.conv2.bias.detach().numpy()))\n",
    "bias = model.conv2.bias.detach().numpy() * 128\n",
    "print(bias)\n",
    "\n",
    "print(\"Convolution Output\")\n",
    "print(np.shape(model.conv2_out_np))\n",
    "output = model.conv2_out_np * 128\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 12, 12)\n",
      "(3, 3, 5, 5)\n",
      "(1, 3, 8, 8)\n",
      "\n",
      "Bias : \n",
      "[-4. -6.  3.]\n",
      "\n",
      "Calc Value :\n",
      "\n",
      "[[[ -106.    90.   250.   563.   928.   929.   377.  -251.]\n",
      "  [   95.   238.   569.  1235.  1621.  1524.   876.   198.]\n",
      "  [   15.   207.   578.  1010.  1288.  1374.  1062.   633.]\n",
      "  [ -210.    73.   534.   561.   502.   590.   786.   868.]\n",
      "  [ -267.   171.   595.   195.   -97.    56.   464.   932.]\n",
      "  [   15.   670.  1154.   609.    61.    40.   396.  1010.]\n",
      "  [  621.  1616.  2085.  1190.   551.   492.   743.  1121.]\n",
      "  [  891.  1848.  2162.  1436.   954.   903.  1092.  1141.]]\n",
      "\n",
      " [[  147.    75.  -150.  -377.  -206.   314.   930.  1112.]\n",
      "  [ -235.  -294.  -434.  -671.  -781.  -492.    57.   635.]\n",
      "  [ -132.  -258.  -328.  -362.  -310.  -373.  -284.   -93.]\n",
      "  [  220.   374.   522.   452.   378.   128.    -8.  -228.]\n",
      "  [  398.   921.  1310.  1186.   798.   411.   135.  -143.]\n",
      "  [  -34.   426.   980.   984.   575.   167.   -74.  -113.]\n",
      "  [ -749.  -695.  -445.  -217.  -185.  -292.  -411.  -273.]\n",
      "  [ -589.  -831. -1024.  -999.  -812.  -630.  -460.  -252.]]\n",
      "\n",
      " [[  -15.   529.   974.   750.   218.    13.   -42.   -64.]\n",
      "  [  504.  1033.  1236.   980.   516.    22.  -228.  -163.]\n",
      "  [  790.  1052.   921.   375.  -123.  -376.  -321.  -125.]\n",
      "  [  577.   489.   -57.  -509.  -465.  -203.   -74.    -4.]\n",
      "  [  347.    27.  -514.  -690.  -183.   439.   406.   153.]\n",
      "  [  407.   -99.  -394.  -104.   776.  1159.   736.   -54.]\n",
      "  [  590.   453.   686.  1143.  1460.  1133.   261.  -423.]\n",
      "  [  954.  1106.  1298.  1288.   985.   244.  -437.  -545.]]]\n",
      "\n",
      "Real Value :\n",
      "\n",
      "[[[[ -106.    90.   250.   563.   928.   929.   377.  -251.]\n",
      "   [   95.   238.   569.  1235.  1621.  1524.   876.   198.]\n",
      "   [   15.   207.   578.  1010.  1288.  1374.  1062.   633.]\n",
      "   [ -210.    73.   534.   561.   502.   590.   786.   868.]\n",
      "   [ -267.   171.   595.   195.   -97.    56.   464.   932.]\n",
      "   [   15.   670.  1154.   609.    61.    40.   396.  1010.]\n",
      "   [  621.  1616.  2085.  1190.   551.   492.   743.  1121.]\n",
      "   [  891.  1848.  2162.  1436.   954.   903.  1092.  1141.]]\n",
      "\n",
      "  [[  147.    75.  -150.  -377.  -206.   314.   930.  1112.]\n",
      "   [ -235.  -294.  -434.  -671.  -781.  -492.    57.   635.]\n",
      "   [ -132.  -258.  -328.  -362.  -310.  -373.  -284.   -93.]\n",
      "   [  220.   374.   522.   452.   378.   128.    -8.  -228.]\n",
      "   [  398.   921.  1310.  1186.   798.   411.   135.  -143.]\n",
      "   [  -34.   426.   980.   984.   575.   167.   -74.  -113.]\n",
      "   [ -749.  -695.  -445.  -217.  -185.  -292.  -411.  -273.]\n",
      "   [ -589.  -831. -1024.  -999.  -812.  -630.  -460.  -252.]]\n",
      "\n",
      "  [[  -15.   529.   974.   750.   218.    13.   -42.   -64.]\n",
      "   [  504.  1033.  1236.   980.   516.    22.  -228.  -163.]\n",
      "   [  790.  1052.   921.   375.  -123.  -376.  -321.  -125.]\n",
      "   [  577.   489.   -57.  -509.  -465.  -203.   -74.    -4.]\n",
      "   [  347.    27.  -514.  -690.  -183.   439.   406.   153.]\n",
      "   [  407.   -99.  -394.  -104.   776.  1159.   736.   -54.]\n",
      "   [  590.   453.   686.  1143.  1460.  1133.   261.  -423.]\n",
      "   [  954.  1106.  1298.  1288.   985.   244.  -437.  -545.]]]]\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(_input))\n",
    "print(np.shape(weight))\n",
    "print(np.shape(output))\n",
    "\n",
    "_input_1 = _input[0]\n",
    "_input_2 = _input[1]\n",
    "_input_3 = _input[2]\n",
    "\n",
    "output_calc_1 = np.zeros((3,8,8))\n",
    "output_calc_2 = np.zeros((3,8,8))\n",
    "output_calc_3 = np.zeros((3,8,8))\n",
    "output_calc = np.zeros((3,8,8))\n",
    "\n",
    "for c in range(3):\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            output_calc_1[c][i][j] += (_input_1[i:i+5, j:j+5] * weight[c][0]).sum()\n",
    "            output_calc_2[c][i][j] += (_input_2[i:i+5, j:j+5] * weight[c][1]).sum()\n",
    "            output_calc_3[c][i][j] += (_input_3[i:i+5, j:j+5] * weight[c][2]).sum()\n",
    "            output_calc[c][i][j] = output_calc_1[c][i][j] + output_calc_2[c][i][j] + output_calc_3[c][i][j] + bias[c] * 128\n",
    "        \n",
    "print(\"\\nBias : \")\n",
    "print(bias)\n",
    "print(\"\\nCalc Value :\\n\")\n",
    "print(output_calc / 128) \n",
    "print(\"\\nReal Value :\\n\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calc Value 1 :\n",
      "\n",
      "[[[ -79563. -115480. -119358.  -83185.  -17614.    8089.  -19178.\n",
      "    -43569.]\n",
      "  [ -76744.  -95861.  -57902.   21259.   71909.   58105.    3008.\n",
      "    -30303.]\n",
      "  [ -77871.  -72264.    -109.   69870.   77375.   31645.   -8140.\n",
      "    -15770.]\n",
      "  [ -86202.  -63898.    8516.   29483.      24.  -51432.  -51033.\n",
      "     -8752.]\n",
      "  [ -99805.  -74930.  -25465.  -50144. -102209. -119513.  -67308.\n",
      "     22131.]\n",
      "  [ -88203.  -55248.  -29550.  -82149. -126760. -100607.  -18583.\n",
      "     77434.]\n",
      "  [ -40519.    7861.   28970.  -29656.  -44251.    1708.   71418.\n",
      "    120942.]\n",
      "  [    207.   57557.   83441.   56935.   63897.   92173.  121985.\n",
      "    118717.]]\n",
      "\n",
      " [[ -34046.  -46650.  -68184.  -87613.  -83399.  -52588.   -9879.\n",
      "     10516.]\n",
      "  [ -49032.  -62741.  -77445.  -91922.  -91984.  -72616.  -38571.\n",
      "     -7807.]\n",
      "  [ -50142.  -56353.  -62881.  -67248.  -65087.  -66195.  -46464.\n",
      "    -24871.]\n",
      "  [ -39451.  -32929.  -31157.  -33193.  -32252.  -46073.  -45074.\n",
      "    -38079.]\n",
      "  [ -42196.  -27861.  -15240.  -17120.  -28000.  -43565.  -47254.\n",
      "    -41824.]\n",
      "  [ -65579.  -56775.  -36730.  -31698.  -49697.  -59595.  -57982.\n",
      "    -39881.]\n",
      "  [ -85973.  -90344.  -82505.  -75706.  -74452.  -69561.  -59637.\n",
      "    -30901.]\n",
      "  [ -66662.  -86434.  -94062.  -89457.  -77806.  -62378.  -39476.\n",
      "    -14770.]]\n",
      "\n",
      " [[ -38486.    6397.   60835.   45844.  -11971.  -58632.  -61148.\n",
      "    -35397.]\n",
      "  [  22569.   78118.  108770.   77235.   16527.  -54760.  -84750.\n",
      "    -59476.]\n",
      "  [  56531.   84800.   82467.   27848.  -25870.  -76233.  -79268.\n",
      "    -53509.]\n",
      "  [  39395.   31797.  -22594.  -75958.  -80973.  -51494.  -30267.\n",
      "    -18183.]\n",
      "  [   5579.  -54266. -123945. -127460.  -51183.   23978.   25844.\n",
      "      6909.]\n",
      "  [  -5916.  -82415. -115566.  -58365.   48530.   89494.   52990.\n",
      "     -6654.]\n",
      "  [  31453.   -5761.    -686.   68322.  119041.   96222.   14028.\n",
      "    -43055.]\n",
      "  [  76656.   76675.   87689.  108970.   93397.   21664.  -50750.\n",
      "    -61677.]]]\n",
      "\n",
      "Calc Value 2 :\n",
      "\n",
      "[[[ 45023.  79466.  75835.  55328.  30290.  19699.   9228. -10690.]\n",
      "  [ 51323.  60357.  31731.  13960.   8799.  19032.  12803.  -2882.]\n",
      "  [ 37552.  28601. -12539. -34950. -18572.  22370.  28756.   6759.]\n",
      "  [ 17666.   9621.  -7583. -20525.  -6994.  38394.  41780.  17059.]\n",
      "  [ 21433.  30710.  39205.  29735.  36684.  56781.  32740.  -3671.]\n",
      "  [ 33873.  59163.  92925.  84741.  63609.  32811. -16758. -34334.]\n",
      "  [ 46021.  84699. 110469.  68047.  16957. -26652. -62165. -53548.]\n",
      "  [ 39617.  63118.  62979.   2968. -51294. -71948. -64274. -35421.]]\n",
      "\n",
      " [[ 45343.  43920.  36116.  29940.  46025.  77925. 106012. 100425.]\n",
      "  [ 13476.  14788.  11718.   1124.  -5787.  16354.  48718.  73157.]\n",
      "  [ 20053.  10483.   7578.   8669.  15039.  22920.  22102.  19288.]\n",
      "  [ 47273.  58394.  70044.  62349.  62499.  51914.  39476.  10211.]\n",
      "  [ 71224. 116078. 145006. 134875. 103137.  70052.  41888.  17925.]\n",
      "  [ 52544.  93379. 130734. 127313.  98415.  58462.  30995.  15071.]\n",
      "  [  2063.  10601.  27907.  42373.  38786.  21170.   2458.  -6607.]\n",
      "  [ -1064.  -5235. -13180. -15772. -14474. -10434. -13943. -13238.]]\n",
      "\n",
      " [[ 30168.  48463.  52512.  54781.  57310.  76070.  76952.  59881.]\n",
      "  [ 28763.  35622.  34823.  38483.  45248.  60245.  69512.  64325.]\n",
      "  [ 27253.  27004.  18073.  10299.  11339.  29659.  42738.  52022.]\n",
      "  [ 14474.  13448.  15707.  13857.  17611.  21003.  24066.  28996.]\n",
      "  [ 19173.  41596.  60939.  53377.  43586.  36552.  27374.  18796.]\n",
      "  [ 31922.  52308.  73816.  74540.  71766.  56207.  35569.  11771.]\n",
      "  [ 21516.  45703.  80213.  86904.  78031.  48213.  22743.   1928.]\n",
      "  [ 20938.  37446.  56443.  51550.  32521.   8733.   -783.  -1501.]]]\n",
      "\n",
      "Calc Value 3 :\n",
      "\n",
      "[[[ 21507.  48105.  76038. 100458. 106709.  91610.  58685.  22734.]\n",
      "  [ 38109.  66452.  99575. 123376. 127301. 118519.  96831.  59132.]\n",
      "  [ 42826.  70663.  87155.  94943. 106571. 122408. 115910.  90594.]\n",
      "  [ 42183.  64189.  67970.  63348.  71739.  89112. 110431. 103391.]\n",
      "  [ 44789.  66599.  63024.  45951.  53598.  70426.  94459. 101381.]\n",
      "  [ 56780.  82393.  84836.  75853.  71508.  73472.  86565.  86780.]\n",
      "  [ 74571. 114790. 127931. 114487.  98364.  88482.  86380.  76636.]\n",
      "  [ 74820. 116464. 130873. 124393. 110091.  95862.  82553.  63306.]]\n",
      "\n",
      " [[  8238.  13086.  13667.  10215.  11733.  15620.  23614.  32204.]\n",
      "  [  6256.  11094.  10939.   5610.  -1428.  -5948.  -2150.  16718.]\n",
      "  [ 13910.  13578.  14042.  13058.  11116.  -3707. -11193.  -5508.]\n",
      "  [ 21146.  23138.  28682.  29481.  18934.  11291.   5357.   -563.]\n",
      "  [ 22713.  30438.  38672.  34787.  27822.  26919.  23405.   6338.]\n",
      "  [  9441.  18710.  32233.  31131.  25603.  23285.  18215.  11062.]\n",
      "  [-11157.  -8401.  -1606.   6285.  12740.  11731.   5276.   3313.]\n",
      "  [ -6885. -14005. -23032. -21893. -10859.  -7064.  -4747.  -3483.]]\n",
      "\n",
      " [[  6020.  12517.  10996.  -4925. -17731. -16174. -21517. -33084.]\n",
      "  [ 12876.  18157.  14242.   9371.   3979.  -2993. -14334. -26072.]\n",
      "  [ 16936.  22524.  17013.   9557.  -1557.  -1948.  -4940. -14813.]\n",
      "  [ 19580.  16972.   -822.  -3374.   3436.   4133.  -3674. -11679.]\n",
      "  [ 19354.  15711.  -3102. -14585. -16193.  -4689.  -1656.  -6511.]\n",
      "  [ 25738.  17053.  -9009. -29900. -21370.   2273.   5260. -12340.]\n",
      "  [ 22163.  17630.   7921.  -9300. -10502.    258.  -3651. -13363.]\n",
      "  [ 24108.  27084.  21717.   4019.   -243.    533.  -4735.  -6987.]]]\n",
      "Sum\n",
      "[[[ -13034.   12091.   32515.   72600.  119384.  119397.   48735.\n",
      "    -31524.]\n",
      "  [  12688.   30948.   73404.  158595.  208010.  195656.  112642.\n",
      "     25947.]\n",
      "  [   2508.   27000.   74506.  129864.  165374.  176423.  136526.\n",
      "     81583.]\n",
      "  [ -26353.    9912.   68902.   72306.   64769.   76074.  101178.\n",
      "    111698.]\n",
      "  [ -33583.   22379.   76764.   25543.  -11927.    7694.   59892.\n",
      "    119841.]\n",
      "  [   2451.   86309.  148211.   78445.    8357.    5677.   51223.\n",
      "    129880.]\n",
      "  [  80073.  207350.  267370.  152878.   71069.   63537.   95633.\n",
      "    144031.]\n",
      "  [ 114643.  237139.  277293.  184296.  122695.  116087.  140264.\n",
      "    146602.]]\n",
      "\n",
      " [[  19535.   10357.  -18402.  -47458.  -25641.   40957.  119747.\n",
      "    143145.]\n",
      "  [ -29301.  -36859.  -54788.  -85188.  -99198.  -62210.    7997.\n",
      "     82068.]\n",
      "  [ -16178.  -32291.  -41261.  -45521.  -38932.  -46982.  -35554.\n",
      "    -11091.]\n",
      "  [  28968.   48603.   67569.   58637.   49181.   17132.    -241.\n",
      "    -28432.]\n",
      "  [  51741.  118655.  168437.  152542.  102958.   53407.   18039.\n",
      "    -17561.]\n",
      "  [  -3594.   55314.  126237.  126747.   74322.   22152.   -8772.\n",
      "    -13748.]\n",
      "  [ -95066.  -88144.  -56204.  -27049.  -22926.  -36660.  -51904.\n",
      "    -34196.]\n",
      "  [ -74611. -105675. -130275. -127122. -103140.  -79876.  -58165.\n",
      "    -31491.]]\n",
      "\n",
      " [[  -2298.   67377.  124343.   95700.   27608.    1264.   -5713.\n",
      "     -8599.]\n",
      "  [  64208.  131897.  157834.  125089.   65755.    2492.  -29572.\n",
      "    -21223.]\n",
      "  [ 100719.  134328.  117553.   47704.  -16088.  -48523.  -41469.\n",
      "    -16300.]\n",
      "  [  73448.   62218.   -7709.  -65476.  -59926.  -26358.   -9876.\n",
      "      -866.]\n",
      "  [  44107.    3042.  -66109.  -88667.  -23789.   55841.   51561.\n",
      "     19194.]\n",
      "  [  51745.  -13054.  -50759.  -13725.   98927.  147974.   93818.\n",
      "     -7224.]\n",
      "  [  75133.   57572.   87448.  145926.  186570.  144693.   33119.\n",
      "    -54491.]\n",
      "  [ 121702.  141205.  165849.  164539.  125674.   30929.  -56268.\n",
      "    -70165.]]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=0)\n",
    "print(\"\\nCalc Value 1 :\\n\")\n",
    "print(output_calc_1) \n",
    "\n",
    "print(\"\\nCalc Value 2 :\\n\")\n",
    "print(output_calc_2) \n",
    "\n",
    "print(\"\\nCalc Value 3 :\\n\")\n",
    "print(output_calc_3) \n",
    "\n",
    "print(\"Sum\")\n",
    "print(output_calc_1 + output_calc_2 + output_calc_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
